channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
channels total 3
Observation space shape (84, 168, 3)
spawn_point_random True single_goal False
using <class 'myPPO.myPPO.myPPO'> with 5 epochs, 64 batch size and 128 steps per epoch
Using cpu device
Wrapping the env in a VecTransposeImage.
observations dtype: uint8
Logging to ./tmp\PPO_187
collect rollouts started
observations dtype: uint8
insertpos: 0
insertpos: 1
insertpos: 2
insertpos: 3
insertpos: 4
insertpos: 5
insertpos: 6
insertpos: 7
insertpos: 8
insertpos: 9
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 2: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 4: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 5: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 6: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 8: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}, 9: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}
info for index 9: {'endEvent': 'WallHit', 'duration': '6.979994', 'cumreward': '7.824747', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-39.4774', 'orientationReward': '48.30215', 'otherReward': '-1', 'velocityReward': '1.18388E-06', 'step': '9', 'amount_of_steps': '10', 'amount_of_steps_based_on_rewardlist': '10', 'bootstrapped_rewards': [1.65043092, -0.7145411, 0.883131444, 1.11827409, -0.0352957733, -0.352202743, 0.8264769, 0.682572842, 0.866093338, 0.0], 'episode': {'r': 7.824749, 'l': 10, 't': 11.256606}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [128, 128, 128, ..., 140, 140, 140],
        [127, 127, 127, ..., 139, 139, 139],
        [127, 127, 127, ..., 139, 139, 139]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [129, 129, 129, ..., 140, 140, 140],
        [128, 128, 128, ..., 139, 140, 140],
        [128, 128, 128, ..., 139, 139, 139]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [132, 132, 132, ..., 140, 141, 141],
        [132, 132, 132, ..., 140, 140, 140],
        [131, 131, 131, ..., 139, 140, 140]]], dtype=uint8)}
insertpos: 10
insertpos: 11
insertpos: 12
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 2: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 4: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 5: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 6: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 8: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}, 9: {0: 10, 1: 11, 2: 12}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12}
info for index 6: {'endEvent': 'WallHit', 'duration': '8.960015', 'cumreward': '14.89255', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-40.35242', 'orientationReward': '56.24499', 'otherReward': '-1', 'velocityReward': '1.375466E-06', 'step': '12', 'amount_of_steps': '13', 'amount_of_steps_based_on_rewardlist': '13', 'bootstrapped_rewards': [0.7935061, 1.259361, -0.306640267, -1.22154593, -1.03163016, 0.8529584, 0.916870058, 2.20648956, 1.41575825, 0.603179753, 1.14242136, 0.132154331, 0.0], 'episode': {'r': 14.892544, 'l': 13, 't': 13.452672}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [188, 188, 187, ..., 181, 181, 181],
        [187, 187, 187, ..., 181, 181, 181],
        [187, 187, 187, ..., 181, 181, 181]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [188, 187, 187, ..., 181, 181, 181],
        [187, 187, 187, ..., 181, 181, 181],
        [187, 187, 187, ..., 181, 181, 181]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [188, 187, 187, ..., 180, 180, 180],
        [187, 187, 187, ..., 180, 180, 180],
        [187, 187, 187, ..., 180, 180, 180]]], dtype=uint8)}
insertpos: 13
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 2: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 4: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 5: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 6: {0: 13}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 8: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 9: {0: 10, 1: 11, 2: 12, 3: 13}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}
info for index 2: {'endEvent': 'WallHit', 'duration': '10.22004', 'cumreward': '24.04943', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-48.16275', 'orientationReward': '73.21219', 'otherReward': '-1', 'velocityReward': '1.458485E-06', 'step': '13', 'amount_of_steps': '14', 'amount_of_steps_based_on_rewardlist': '14', 'bootstrapped_rewards': [1.06627083, -1.37383938, -1.5968492, 1.6926, 1.00813985, -0.157741457, 0.495889157, 1.40879762, 1.36855233, 1.93579233, 1.67454982, 1.14182961, 0.676215947, 0.0], 'episode': {'r': 24.049439, 'l': 14, 't': 14.166907}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 178, 178, 178],
        [169, 169, 169, ..., 177, 177, 177],
        [169, 168, 168, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 178, 178, 178],
        [168, 168, 168, ..., 178, 178, 178],
        [168, 168, 168, ..., 177, 177, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [168, 168, 168, ..., 179, 179, 179],
        [168, 168, 168, ..., 178, 178, 179],
        [168, 168, 168, ..., 178, 178, 178]]], dtype=uint8)}
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 2: {}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 4: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 5: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 6: {0: 13}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 8: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 9: {0: 10, 1: 11, 2: 12, 3: 13}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}
info for index 4: {'endEvent': 'WallHit', 'duration': '10.02004', 'cumreward': '33.17812', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-15.59708', 'orientationReward': '49.74397', 'otherReward': '-1', 'velocityReward': '0.03122929', 'step': '13', 'amount_of_steps': '14', 'amount_of_steps_based_on_rewardlist': '14', 'bootstrapped_rewards': [5.511158, 3.82656074, 4.06864166, 0.8354699, -0.599741161, 0.470778048, 2.80752683, 1.81512439, 0.5730424, 0.9608626, -0.0214425549, 0.940616548, 0.434345424, 0.0], 'episode': {'r': 33.178118, 'l': 14, 't': 14.248569}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [150, 150, 150, ..., 157, 157, 157],
        [150, 150, 150, ..., 157, 157, 157],
        [150, 150, 150, ..., 156, 157, 157]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [150, 150, 150, ..., 158, 158, 158],
        [150, 150, 150, ..., 157, 157, 157],
        [150, 150, 150, ..., 157, 157, 157]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [151, 151, 151, ..., 158, 158, 158],
        [151, 151, 151, ..., 158, 158, 158],
        [150, 150, 150, ..., 157, 157, 157]]], dtype=uint8)}
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 2: {}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 4: {}, 5: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 6: {0: 13}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 8: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}, 9: {0: 10, 1: 11, 2: 12, 3: 13}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13}
info for index 5: {'endEvent': 'WallHit', 'duration': '9.840035', 'cumreward': '22.1052', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-44.66227', 'orientationReward': '67.76749', 'otherReward': '-1', 'velocityReward': '1.281566E-06', 'step': '13', 'amount_of_steps': '14', 'amount_of_steps_based_on_rewardlist': '14', 'bootstrapped_rewards': [1.78741455, -0.8053612, 3.44845128, 1.2057277, 1.08839989, 0.8187663, -0.2650252, 0.8631464, 0.9255604, 0.2625021, 0.723298669, 1.48409545, 0.372397333, 0.0], 'episode': {'r': 22.105197, 'l': 14, 't': 14.305723}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [165, 165, 165, ..., 174, 174, 174],
        [165, 165, 165, ..., 174, 174, 174],
        [165, 165, 165, ..., 173, 173, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [165, 165, 165, ..., 174, 174, 174],
        [165, 165, 165, ..., 174, 174, 174],
        [165, 165, 165, ..., 173, 173, 174]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [166, 166, 166, ..., 175, 175, 175],
        [166, 166, 166, ..., 174, 174, 175],
        [166, 166, 166, ..., 174, 174, 174]]], dtype=uint8)}
insertpos: 14
insertpos: 15
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}, 2: {0: 14, 1: 15}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}, 4: {0: 14, 1: 15}, 5: {0: 14, 1: 15}, 6: {0: 13, 1: 14, 2: 15}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}, 8: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15}
info for index 8: {'endEvent': 'WallHit', 'duration': '11.38007', 'cumreward': '19.26516', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-53.09184', 'orientationReward': '73.35696', 'otherReward': '-1', 'velocityReward': '2.038947E-05', 'step': '15', 'amount_of_steps': '16', 'amount_of_steps_based_on_rewardlist': '16', 'bootstrapped_rewards': [2.6873157, 0.2765654, 0.5470579, 2.94832468, 0.478617638, -0.8777562, -0.8643778, 1.96053433, 1.14474356, 1.15645289, 0.8532645, 0.274259835, 0.179590449, 0.182451919, -0.110674568, 0.0], 'episode': {'r': 19.265143, 'l': 16, 't': 15.85406}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 175, 175, 175],
        [169, 169, 169, ..., 175, 175, 175],
        [169, 168, 168, ..., 174, 175, 175]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 175, 175, 175],
        [169, 169, 169, ..., 175, 175, 175],
        [169, 169, 169, ..., 175, 175, 175]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [170, 170, 170, ..., 176, 176, 176],
        [170, 170, 170, ..., 176, 176, 176],
        [170, 170, 170, ..., 175, 176, 176]]], dtype=uint8)}
insertpos: 16
insertpos: 17
insertpos: 18
insertpos: 19
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 2: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}, 3: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 4: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}, 6: {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 8: {0: 16, 1: 17, 2: 18, 3: 19}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}
info for index 3: {'endEvent': 'WallHit', 'duration': '14.50014', 'cumreward': '58.2449', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-55.48874', 'orientationReward': '114.7336', 'otherReward': '-1', 'velocityReward': '6.45554E-05', 'step': '19', 'amount_of_steps': '20', 'amount_of_steps_based_on_rewardlist': '20', 'bootstrapped_rewards': [5.064783, 1.41702843, 0.3042363, -0.0404098257, -0.341203749, 2.14715147, 2.340576, 0.9520177, 0.106776468, 0.848896742, 0.7080284, 0.855157733, 1.13136339, 1.44201994, 0.888792634, 1.21871328, 1.2388128, 1.23532879, 0.1506719, 0.0], 'episode': {'r': 58.244895, 'l': 20, 't': 18.977919}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [168, 168, 168, ..., 176, 176, 176],
        [168, 168, 168, ..., 176, 176, 176],
        [167, 167, 167, ..., 175, 176, 176]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [168, 168, 168, ..., 176, 176, 176],
        [168, 168, 168, ..., 176, 176, 176],
        [167, 167, 167, ..., 175, 175, 176]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [168, 168, 168, ..., 176, 176, 176],
        [168, 168, 168, ..., 176, 176, 176],
        [167, 167, 167, ..., 175, 176, 176]]], dtype=uint8)}
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 2: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}, 3: {}, 4: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}, 6: {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19}, 8: {0: 16, 1: 17, 2: 18, 3: 19}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19}}
reward correction dict entry {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19}
info for index 4: {'endEvent': 'WallHit', 'duration': '3.859997', 'cumreward': '18.1733', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-8.924532', 'orientationReward': '28.09783', 'otherReward': '-1', 'velocityReward': '0', 'step': '5', 'amount_of_steps': '6', 'amount_of_steps_based_on_rewardlist': '6', 'bootstrapped_rewards': [4.736322, 3.83281612, 2.91328454, 2.65456414, 1.27301049, 0.0], 'episode': {'r': 18.173302, 'l': 6, 't': 19.038387}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ...,  19,  21,  22],
        [249, 249, 249, ...,  19,  21,  22],
        [249, 249, 249, ...,  19,  22,  22],
        ...,
        [152, 152, 152, ...,  24,  92, 114],
        [152, 152, 152, ...,  24,  92, 114],
        [151, 151, 151, ...,  24,  91, 114]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [155, 155, 155, ..., 151, 151, 151],
        [155, 155, 154, ..., 151, 151, 151],
        [154, 154, 154, ..., 151, 151, 151]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [158, 158, 158, ..., 153, 154, 154],
        [158, 158, 158, ..., 153, 153, 153],
        [157, 157, 157, ..., 153, 153, 153]]], dtype=uint8)}
insertpos: 20
insertpos: 21
insertpos: 22
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}, 1: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}, 2: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22}, 3: {0: 20, 1: 21, 2: 22}, 4: {0: 20, 1: 21, 2: 22}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22}, 6: {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19, 7: 20, 8: 21, 9: 22}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}
info for index 1: {'endEvent': 'WallHit', 'duration': '16.82019', 'cumreward': '85.47807', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-47.96495', 'orientationReward': '134.4431', 'otherReward': '-1', 'velocityReward': '1.455913E-06', 'step': '22', 'amount_of_steps': '23', 'amount_of_steps_based_on_rewardlist': '23', 'bootstrapped_rewards': [3.02124786, 2.08089662, 1.140669, 2.98956919, 1.14350617, 1.52408791, 2.690249, 2.62321186, 1.55158138, 1.47421515, 0.871676, 0.4165384, 1.38196492, 1.62114716, 1.10401642, 0.9632334, 0.6531482, 0.725906134, 0.8357022, 0.7197409, 0.8558178, 0.212290317, 0.0], 'episode': {'r': 85.478096, 'l': 23, 't': 21.173255}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 178, 179, 179],
        [169, 169, 169, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 178, 179, 179],
        [169, 169, 169, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 178, 178, 178]]], dtype=uint8)}
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}, 1: {}, 2: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22}, 3: {0: 20, 1: 21, 2: 22}, 4: {0: 20, 1: 21, 2: 22}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22}, 6: {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19, 7: 20, 8: 21, 9: 22}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22}}
reward correction dict entry {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22}
info for index 2: {'endEvent': 'WallHit', 'duration': '5.619996', 'cumreward': '5.706567', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-33.80184', 'orientationReward': '40.50841', 'otherReward': '-1', 'velocityReward': '0', 'step': '8', 'amount_of_steps': '9', 'amount_of_steps_based_on_rewardlist': '9', 'bootstrapped_rewards': [3.24931049, 0.9914217, 0.168047413, 0.0594314672, 0.1521188, 0.3071083, 0.388373882, -0.198996544, 0.0], 'episode': {'r': 5.706566, 'l': 9, 't': 21.242252}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [161, 161, 161, ..., 170, 170, 171],
        [161, 161, 161, ..., 170, 170, 170],
        [161, 161, 161, ..., 169, 169, 169]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [161, 161, 161, ..., 170, 171, 171],
        [161, 161, 161, ..., 170, 170, 170],
        [161, 161, 161, ..., 169, 169, 170]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [163, 163, 163, ..., 171, 171, 171],
        [163, 163, 163, ..., 171, 171, 171],
        [163, 163, 163, ..., 170, 170, 170]]], dtype=uint8)}
insertpos: 23
reward correction dict: {0: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23}, 1: {0: 23}, 2: {0: 23}, 3: {0: 20, 1: 21, 2: 22, 3: 23}, 4: {0: 20, 1: 21, 2: 22, 3: 23}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23}, 6: {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19, 7: 20, 8: 21, 9: 22, 10: 23}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22, 7: 23}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22, 13: 23}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23}
info for index 0: {'endEvent': 'WallHit', 'duration': '17.76022', 'cumreward': '60.56927', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-101.4158', 'orientationReward': '162.9852', 'otherReward': '-1', 'velocityReward': '1.508363E-06', 'step': '23', 'amount_of_steps': '24', 'amount_of_steps_based_on_rewardlist': '24', 'bootstrapped_rewards': [2.26917362, 1.197487, 0.4760874, 0.123339854, 0.8910437, 1.21972, 2.389927, 0.511015952, 0.1863755, -0.00354087213, 0.0312406067, 0.143324688, 0.264239132, 0.438890517, 0.847165465, 1.71711242, 1.03141475, 0.6784575, 0.5495577, 0.89518255, 0.61709106, 0.486605078, 0.207055569, 0.0], 'episode': {'r': 60.569317, 'l': 24, 't': 22.004081}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [142, 142, 142, ..., 146, 146, 146],
        [141, 141, 141, ..., 145, 145, 146],
        [141, 141, 141, ..., 145, 145, 145]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [142, 142, 142, ..., 146, 146, 146],
        [142, 141, 142, ..., 146, 146, 146],
        [141, 141, 141, ..., 145, 145, 145]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [144, 144, 144, ..., 149, 149, 149],
        [144, 144, 144, ..., 149, 149, 149],
        [144, 144, 144, ..., 149, 149, 149]]], dtype=uint8)}
insertpos: 24
insertpos: 25
reward correction dict: {0: {0: 24, 1: 25}, 1: {0: 23, 1: 24, 2: 25}, 2: {0: 23, 1: 24, 2: 25}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25}, 6: {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19, 7: 20, 8: 21, 9: 22, 10: 23, 11: 24, 12: 25}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22, 7: 23, 8: 24, 9: 25}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22, 13: 23, 14: 24, 15: 25}}
reward correction dict entry {0: 13, 1: 14, 2: 15, 3: 16, 4: 17, 5: 18, 6: 19, 7: 20, 8: 21, 9: 22, 10: 23, 11: 24, 12: 25}
info for index 6: {'endEvent': 'WallHit', 'duration': '8.820012', 'cumreward': '17.30386', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-46.29285', 'orientationReward': '64.59673', 'otherReward': '-1', 'velocityReward': '1.56296E-06', 'step': '12', 'amount_of_steps': '13', 'amount_of_steps_based_on_rewardlist': '13', 'bootstrapped_rewards': [2.246693, 0.411711365, -0.0460447147, 0.372865558, 0.5045681, 1.76140559, 1.12234712, 0.8819722, 0.916009545, 0.8548234, 0.742450356, -0.008318892, 0.0], 'episode': {'r': 17.303868, 'l': 13, 't': 23.670674}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [162, 162, 162, ..., 175, 175, 175],
        [162, 162, 162, ..., 174, 175, 175],
        [162, 162, 162, ..., 174, 174, 174]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [162, 162, 162, ..., 175, 175, 175],
        [162, 162, 162, ..., 174, 175, 175],
        [162, 162, 162, ..., 174, 174, 174]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [163, 163, 163, ..., 176, 176, 176],
        [163, 163, 163, ..., 175, 176, 176],
        [163, 163, 163, ..., 175, 175, 175]]], dtype=uint8)}
reward correction dict: {0: {0: 24, 1: 25}, 1: {0: 23, 1: 24, 2: 25}, 2: {0: 23, 1: 24, 2: 25}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25}, 6: {}, 7: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22, 7: 23, 8: 24, 9: 25}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22, 13: 23, 14: 24, 15: 25}}
reward correction dict entry {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25}
info for index 7: {'endEvent': 'WallHit', 'duration': '19.06025', 'cumreward': '90.91225', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-87.81177', 'orientationReward': '179.7241', 'otherReward': '-1', 'velocityReward': '1.328829E-06', 'step': '25', 'amount_of_steps': '26', 'amount_of_steps_based_on_rewardlist': '26', 'bootstrapped_rewards': [1.94676471, 1.46931636, 0.7982288, 0.9869577, -0.019924501, 2.82577157, 2.99007273, 1.26889825, 0.202623367, 0.0472257957, 0.885862231, 0.6611142, 0.6477364, 0.533535957, 0.927469969, 1.25543749, 1.18944, 1.153288, 1.45641685, 1.157028, 0.767348, 0.5732952, 0.453594625, 0.3979692, -0.00600327, 0.0], 'episode': {'r': 90.91224, 'l': 26, 't': 23.72802}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 174, 174, ..., 177, 177, 177],
        [174, 174, 174, ..., 177, 177, 177],
        [174, 174, 174, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 174, 174, ..., 177, 177, 177],
        [174, 174, 174, ..., 177, 177, 177],
        [174, 174, 174, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [175, 175, 175, ..., 177, 177, 178],
        [174, 174, 174, ..., 177, 177, 177],
        [174, 174, 174, ..., 177, 177, 177]]], dtype=uint8)}
reward correction dict: {0: {0: 24, 1: 25}, 1: {0: 23, 1: 24, 2: 25}, 2: {0: 23, 1: 24, 2: 25}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25}, 6: {}, 7: {}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22, 7: 23, 8: 24, 9: 25}, 9: {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22, 13: 23, 14: 24, 15: 25}}
reward correction dict entry {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19, 10: 20, 11: 21, 12: 22, 13: 23, 14: 24, 15: 25}
info for index 9: {'endEvent': 'WallHit', 'duration': '11.74008', 'cumreward': '29.94408', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-54.4513', 'orientationReward': '85.39537', 'otherReward': '-1', 'velocityReward': '0', 'step': '15', 'amount_of_steps': '16', 'amount_of_steps_based_on_rewardlist': '16', 'bootstrapped_rewards': [1.58016682, 1.72920036, 1.21472061, 0.5530091, 0.468959153, 1.59408879, 1.49400294, 0.6027917, 0.35033077, 0.276788384, 0.746305, 1.1154505, 1.05601, 0.836252034, 0.5596426, 0.0], 'episode': {'r': 29.9441, 'l': 16, 't': 23.799056}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [134, 134, 134, ..., 143, 143, 143],
        [134, 134, 134, ..., 142, 142, 143],
        [134, 134, 134, ..., 142, 142, 142]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [135, 135, 135, ..., 144, 144, 144],
        [135, 135, 135, ..., 144, 144, 144],
        [135, 135, 135, ..., 143, 144, 144]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [136, 136, 136, ..., 146, 146, 146],
        [136, 136, 136, ..., 145, 145, 145],
        [136, 136, 136, ..., 145, 145, 145]]], dtype=uint8)}
insertpos: 26
reward correction dict: {0: {0: 24, 1: 25, 2: 26}, 1: {0: 23, 1: 24, 2: 25, 3: 26}, 2: {0: 23, 1: 24, 2: 25, 3: 26}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25, 12: 26}, 6: {0: 26}, 7: {0: 26}, 8: {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22, 7: 23, 8: 24, 9: 25, 10: 26}, 9: {0: 26}}
reward correction dict entry {0: 16, 1: 17, 2: 18, 3: 19, 4: 20, 5: 21, 6: 22, 7: 23, 8: 24, 9: 25, 10: 26}
info for index 8: {'endEvent': 'WallHit', 'duration': '7.779994', 'cumreward': '7.414377', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-44.70585', 'orientationReward': '53.12023', 'otherReward': '-1', 'velocityReward': '0', 'step': '10', 'amount_of_steps': '11', 'amount_of_steps_based_on_rewardlist': '11', 'bootstrapped_rewards': [1.30502272, -1.21247959, 0.306445628, 1.31009114, 1.22402775, 0.319338143, 0.08600573, 0.380446017, 0.71152097, 0.102493018, 0.0], 'episode': {'r': 7.414382, 'l': 11, 't': 24.583973}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [187, 187, 187, ..., 178, 178, 178],
        [187, 186, 186, ..., 177, 178, 177],
        [186, 186, 186, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ...,  69, 149, 224],
        [249, 249, 249, ...,  44,  49,  74],
        ...,
        [188, 187, 187, ..., 179, 179, 179],
        [187, 187, 187, ..., 178, 178, 178],
        [187, 187, 186, ..., 178, 178, 178]],

       [[249, 249, 249, ...,  64,  38,  38],
        [249, 249, 249, ...,  38,  38,  38],
        [249, 249, 249, ...,  38,  38,  38],
        ...,
        [189, 189, 188, ..., 180, 180, 180],
        [188, 188, 188, ..., 180, 180, 180],
        [188, 188, 188, ..., 180, 180, 180]]], dtype=uint8)}
insertpos: 27
reward correction dict: {0: {0: 24, 1: 25, 2: 26, 3: 27}, 1: {0: 23, 1: 24, 2: 25, 3: 26, 4: 27}, 2: {0: 23, 1: 24, 2: 25, 3: 26, 4: 27}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25, 12: 26, 13: 27}, 6: {0: 26, 1: 27}, 7: {0: 26, 1: 27}, 8: {0: 27}, 9: {0: 26, 1: 27}}
reward correction dict entry {0: 23, 1: 24, 2: 25, 3: 26, 4: 27}
info for index 1: {'endEvent': 'WallHit', 'duration': '3.019998', 'cumreward': '9.119011', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-10.84625', 'orientationReward': '20.96432', 'otherReward': '-1', 'velocityReward': '0.0009377281', 'step': '4', 'amount_of_steps': '5', 'amount_of_steps_based_on_rewardlist': '5', 'bootstrapped_rewards': [2.580499, 3.304975, 2.765627, -0.399517268, 0.0], 'episode': {'r': 9.119013, 'l': 5, 't': 25.266442}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [170, 170, 170, ..., 175, 175, 176],
        [170, 170, 170, ..., 175, 175, 175],
        [170, 170, 170, ..., 175, 175, 175]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [171, 171, 171, ..., 176, 176, 176],
        [171, 171, 171, ..., 176, 176, 176],
        [171, 171, 171, ..., 175, 175, 175]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 173, 173, ..., 178, 178, 178],
        [173, 173, 173, ..., 177, 177, 177],
        [173, 173, 173, ..., 177, 177, 177]]], dtype=uint8)}
reward correction dict: {0: {0: 24, 1: 25, 2: 26, 3: 27}, 1: {}, 2: {0: 23, 1: 24, 2: 25, 3: 26, 4: 27}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25, 12: 26, 13: 27}, 6: {0: 26, 1: 27}, 7: {0: 26, 1: 27}, 8: {0: 27}, 9: {0: 26, 1: 27}}
reward correction dict entry {0: 23, 1: 24, 2: 25, 3: 26, 4: 27}
info for index 2: {'endEvent': 'WallHit', 'duration': '2.919998', 'cumreward': '10.60312', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-9.546752', 'orientationReward': '21.1345', 'otherReward': '-1', 'velocityReward': '0.01536767', 'step': '4', 'amount_of_steps': '5', 'amount_of_steps_based_on_rewardlist': '5', 'bootstrapped_rewards': [6.85326672, 3.63997221, 0.612481952, -0.765966952, 0.0], 'episode': {'r': 10.603122, 'l': 5, 't': 25.335908}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [182, 182, 182, ..., 179, 179, 179],
        [182, 182, 182, ..., 179, 179, 179],
        [181, 181, 181, ..., 178, 179, 179]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [182, 182, 182, ..., 180, 180, 180],
        [182, 182, 182, ..., 180, 180, 180],
        [182, 182, 181, ..., 179, 179, 179]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [184, 184, 184, ..., 183, 183, 183],
        [183, 183, 183, ..., 183, 183, 183],
        [183, 183, 183, ..., 182, 182, 182]]], dtype=uint8)}
insertpos: 28
reward correction dict: {0: {0: 24, 1: 25, 2: 26, 3: 27, 4: 28}, 1: {0: 28}, 2: {0: 28}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28}, 5: {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25, 12: 26, 13: 27, 14: 28}, 6: {0: 26, 1: 27, 2: 28}, 7: {0: 26, 1: 27, 2: 28}, 8: {0: 27, 1: 28}, 9: {0: 26, 1: 27, 2: 28}}
reward correction dict entry {0: 14, 1: 15, 2: 16, 3: 17, 4: 18, 5: 19, 6: 20, 7: 21, 8: 22, 9: 23, 10: 24, 11: 25, 12: 26, 13: 27, 14: 28}
info for index 5: {'endEvent': 'WallHit', 'duration': '10.96006', 'cumreward': '21.94083', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-47.88828', 'orientationReward': '70.8291', 'otherReward': '-1', 'velocityReward': '0', 'step': '14', 'amount_of_steps': '15', 'amount_of_steps_based_on_rewardlist': '15', 'bootstrapped_rewards': [3.223586, 2.10460472, 0.6353394, 2.710134, 0.62761265, -0.681665361, 0.385716349, 0.247001529, 0.775469, 0.8331385, 0.485975951, 0.4317962, 0.637369454, 0.471264362, 0.0], 'episode': {'r': 21.940826, 'l': 15, 't': 26.166711}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [150, 150, 150, ..., 161, 161, 161],
        [150, 150, 150, ..., 161, 161, 161],
        [150, 150, 150, ..., 160, 160, 160]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [151, 151, 151, ..., 163, 163, 163],
        [151, 151, 151, ..., 162, 162, 163],
        [151, 151, 151, ..., 162, 162, 162]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [153, 153, 153, ..., 165, 165, 165],
        [153, 153, 153, ..., 164, 164, 164],
        [153, 153, 153, ..., 164, 164, 164]]], dtype=uint8)}
insertpos: 29
insertpos: 30
insertpos: 31
insertpos: 32
insertpos: 33
insertpos: 34
insertpos: 35
insertpos: 36
insertpos: 37
reward correction dict: {0: {0: 24, 1: 25, 2: 26, 3: 27, 4: 28, 5: 29, 6: 30, 7: 31, 8: 32, 9: 33, 10: 34, 11: 35, 12: 36, 13: 37}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37}, 5: {0: 29, 1: 30, 2: 31, 3: 32, 4: 33, 5: 34, 6: 35, 7: 36, 8: 37}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37}, 7: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37}, 9: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37}}
reward correction dict entry {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37}
info for index 7: {'endEvent': 'WallHit', 'duration': '7.599994', 'cumreward': '4.557577', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-43.3523', 'orientationReward': '48.9098', 'otherReward': '-1', 'velocityReward': '7.194897E-05', 'step': '11', 'amount_of_steps': '12', 'amount_of_steps_based_on_rewardlist': '12', 'bootstrapped_rewards': [4.04519463, 3.15115881, 0.898468733, -0.7247868, -0.9306893, -0.7966047, -0.5442661, -0.290334433, 0.425612777, 0.460456133, -0.307680875, 0.0], 'episode': {'r': 4.557578, 'l': 12, 't': 32.842489}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [147, 147, 147, ..., 150, 150, 150],
        [147, 147, 147, ..., 150, 150, 150],
        [147, 147, 147, ..., 149, 149, 150]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [147, 147, 147, ..., 150, 150, 150],
        [147, 147, 147, ..., 150, 150, 150],
        [147, 147, 147, ..., 150, 150, 150]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [148, 148, 148, ..., 151, 151, 151],
        [148, 147, 147, ..., 150, 150, 150],
        [147, 147, 147, ..., 150, 150, 150]]], dtype=uint8)}
insertpos: 38
reward correction dict: {0: {0: 24, 1: 25, 2: 26, 3: 27, 4: 28, 5: 29, 6: 30, 7: 31, 8: 32, 9: 33, 10: 34, 11: 35, 12: 36, 13: 37, 14: 38}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38}, 5: {0: 29, 1: 30, 2: 31, 3: 32, 4: 33, 5: 34, 6: 35, 7: 36, 8: 37, 9: 38}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38}, 7: {0: 38}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38}, 9: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38}}
reward correction dict entry {0: 24, 1: 25, 2: 26, 3: 27, 4: 28, 5: 29, 6: 30, 7: 31, 8: 32, 9: 33, 10: 34, 11: 35, 12: 36, 13: 37, 14: 38}
info for index 0: {'endEvent': 'WallHit', 'duration': '10.40005', 'cumreward': '17.77059', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-45.28106', 'orientationReward': '64.05169', 'otherReward': '-1', 'velocityReward': '0', 'step': '14', 'amount_of_steps': '15', 'amount_of_steps_based_on_rewardlist': '15', 'bootstrapped_rewards': [2.93896914, 0.08143234, -1.59175718, 0.9064881, 1.987413, 0.201406375, -0.558851659, 0.851411, 1.2484833, 0.996993065, 0.430227131, 0.2544646, 0.7209201, 0.328670084, 0.0], 'episode': {'r': 17.770584, 'l': 15, 't': 33.493034}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [175, 175, 175, ..., 178, 178, 179],
        [175, 175, 175, ..., 178, 178, 178],
        [175, 175, 175, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 171, 249, 249],
        ...,
        [175, 175, 175, ..., 179, 179, 179],
        [175, 175, 175, ..., 178, 178, 178],
        [175, 175, 175, ..., 178, 178, 178]],

       [[249, 249, 249, ...,  92, 197, 249],
        [249, 249, 249, ...,  39,  40, 119],
        [249, 249, 249, ...,  39,  39,  40],
        ...,
        [176, 175, 175, ..., 179, 179, 179],
        [175, 175, 175, ..., 179, 179, 179],
        [175, 175, 175, ..., 178, 178, 178]]], dtype=uint8)}
reward correction dict: {0: {}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38}, 5: {0: 29, 1: 30, 2: 31, 3: 32, 4: 33, 5: 34, 6: 35, 7: 36, 8: 37, 9: 38}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38}, 7: {0: 38}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38}, 9: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38}}
reward correction dict entry {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38}
info for index 9: {'endEvent': 'WallHit', 'duration': '8.580007', 'cumreward': '27.47766', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-35.63413', 'orientationReward': '64.11179', 'otherReward': '-1', 'velocityReward': '0', 'step': '12', 'amount_of_steps': '13', 'amount_of_steps_based_on_rewardlist': '13', 'bootstrapped_rewards': [1.91115177, 0.620757163, -0.116260208, -0.0342146829, 2.516501, 1.33768356, 1.76450336, 2.33813977, 1.5860858, 0.5117602, 1.685792, 0.28785497, 0.0], 'episode': {'r': 27.477643, 'l': 13, 't': 33.696798}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [103, 103, 103, ..., 109, 109, 109],
        [103, 103, 103, ..., 109, 109, 109],
        [103, 103, 103, ..., 108, 109, 109]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [103, 103, 103, ..., 109, 109, 109],
        [103, 103, 103, ..., 109, 109, 109],
        [103, 103, 103, ..., 109, 109, 109]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [104, 104, 104, ..., 110, 110, 110],
        [104, 104, 104, ..., 109, 109, 109],
        [104, 104, 104, ..., 109, 109, 109]]], dtype=uint8)}
insertpos: 39
reward correction dict: {0: {0: 39}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39}, 5: {0: 29, 1: 30, 2: 31, 3: 32, 4: 33, 5: 34, 6: 35, 7: 36, 8: 37, 9: 38, 10: 39}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39}, 7: {0: 38, 1: 39}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38, 12: 39}, 9: {0: 39}}
reward correction dict entry {0: 29, 1: 30, 2: 31, 3: 32, 4: 33, 5: 34, 6: 35, 7: 36, 8: 37, 9: 38, 10: 39}
info for index 5: {'endEvent': 'WallHit', 'duration': '6.779994', 'cumreward': '13.55139', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-27.52865', 'orientationReward': '42.08004', 'otherReward': '-1', 'velocityReward': '0', 'step': '10', 'amount_of_steps': '11', 'amount_of_steps_based_on_rewardlist': '11', 'bootstrapped_rewards': [1.621901, -0.2994353, -0.878866553, -0.8749979, 3.08178043, 2.01413417, 0.2847545, 1.86821413, 1.19102514, -0.2980036, 0.0], 'episode': {'r': 13.551387, 'l': 11, 't': 34.420744}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [141, 141, 141, ..., 147, 147, 147],
        [141, 141, 141, ..., 147, 147, 147],
        [141, 141, 141, ..., 146, 146, 146]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [141, 141, 141, ..., 147, 147, 147],
        [141, 141, 141, ..., 147, 147, 147],
        [141, 141, 141, ..., 146, 146, 147]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [142, 142, 142, ..., 147, 147, 147],
        [141, 142, 141, ..., 147, 147, 147],
        [141, 141, 141, ..., 147, 147, 147]]], dtype=uint8)}
insertpos: 40
reward correction dict: {0: {0: 39, 1: 40}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40}, 3: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40}, 5: {0: 40}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40}, 7: {0: 38, 1: 39, 2: 40}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38, 12: 39, 13: 40}, 9: {0: 39, 1: 40}}
reward correction dict entry {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40}
info for index 3: {'endEvent': 'WallHit', 'duration': '15.12016', 'cumreward': '104.9331', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-17.16059', 'orientationReward': '123.0938', 'otherReward': '-1', 'velocityReward': '0', 'step': '20', 'amount_of_steps': '21', 'amount_of_steps_based_on_rewardlist': '21', 'bootstrapped_rewards': [4.67508459, 4.54055738, 3.50917721, 2.633449, 2.00879645, 3.575252, 2.83460855, 1.58502185, 1.99696362, 1.50692165, 1.58842552, 0.987137139, 0.9768826, 1.6314038, 1.69341314, 1.07270443, 1.22354627, 1.34180093, 1.53263772, 0.800375, 0.0], 'episode': {'r': 104.933137, 'l': 21, 't': 35.173352}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 179, 180, 180],
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 179, 179, 179]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 179, 179, 180],
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 178, 179, 179]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 179, 179, 180],
        [169, 169, 169, ..., 179, 179, 179],
        [169, 169, 169, ..., 178, 178, 179]]], dtype=uint8)}
insertpos: 41
insertpos: 42
insertpos: 43
insertpos: 44
reward correction dict: {0: {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44}, 3: {0: 41, 1: 42, 2: 43, 3: 44}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40, 21: 41, 22: 42, 23: 43, 24: 44}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38, 12: 39, 13: 40, 14: 41, 15: 42, 16: 43, 17: 44}, 9: {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44}}
reward correction dict entry {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44}
info for index 9: {'endEvent': 'WallHit', 'duration': '3.639997', 'cumreward': '17.676', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-6.746831', 'orientationReward': '25.42284', 'otherReward': '-1', 'velocityReward': '0', 'step': '5', 'amount_of_steps': '6', 'amount_of_steps_based_on_rewardlist': '6', 'bootstrapped_rewards': [3.16804457, 0.992519855, 4.43928528, 3.36649656, 2.169458, 0.0], 'episode': {'r': 17.676006, 'l': 6, 't': 38.23756}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [129, 129, 129, ..., 134, 134, 134],
        [128, 128, 128, ..., 133, 133, 133],
        [128, 128, 128, ..., 133, 133, 133]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [130, 130, 130, ..., 134, 134, 134],
        [129, 129, 129, ..., 133, 134, 134],
        [129, 129, 129, ..., 133, 133, 133]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [130, 130, 130, ..., 134, 134, 134],
        [130, 130, 130, ..., 133, 133, 134],
        [129, 129, 129, ..., 133, 133, 133]]], dtype=uint8)}
insertpos: 45
insertpos: 46
reward correction dict: {0: {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44, 6: 45, 7: 46}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46}, 3: {0: 41, 1: 42, 2: 43, 3: 44, 4: 45, 5: 46}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40, 21: 41, 22: 42, 23: 43, 24: 44, 25: 45, 26: 46}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46}, 8: {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38, 12: 39, 13: 40, 14: 41, 15: 42, 16: 43, 17: 44, 18: 45, 19: 46}, 9: {0: 45, 1: 46}}
reward correction dict entry {0: 27, 1: 28, 2: 29, 3: 30, 4: 31, 5: 32, 6: 33, 7: 34, 8: 35, 9: 36, 10: 37, 11: 38, 12: 39, 13: 40, 14: 41, 15: 42, 16: 43, 17: 44, 18: 45, 19: 46}
info for index 8: {'endEvent': 'WallHit', 'duration': '14.12013', 'cumreward': '53.10469', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-43.22622', 'orientationReward': '97.33089', 'otherReward': '-1', 'velocityReward': '0', 'step': '19', 'amount_of_steps': '20', 'amount_of_steps_based_on_rewardlist': '20', 'bootstrapped_rewards': [1.42096233, -0.520278633, 0.7182528, 0.9404088, 0.078796044, 0.7262781, 1.327918, 0.9936587, 1.32244658, 1.37284672, 1.65804827, 1.22544265, 1.18090117, 0.9410524, 1.140845, 0.9097462, 0.948282063, 0.749698758, 0.297353983, 0.0], 'episode': {'r': 53.104719, 'l': 20, 't': 39.752507}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [188, 188, 188, ..., 182, 182, 182],
        [187, 187, 187, ..., 182, 182, 182],
        [187, 187, 187, ..., 182, 182, 182]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [189, 189, 189, ..., 183, 183, 183],
        [188, 188, 188, ..., 183, 183, 183],
        [188, 188, 188, ..., 183, 183, 183]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [190, 190, 190, ..., 185, 184, 185],
        [190, 190, 190, ..., 184, 184, 184],
        [189, 189, 189, ..., 184, 184, 184]]], dtype=uint8)}
insertpos: 47
reward correction dict: {0: {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44, 6: 45, 7: 46, 8: 47}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47}, 3: {0: 41, 1: 42, 2: 43, 3: 44, 4: 45, 5: 46, 6: 47}, 4: {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40, 21: 41, 22: 42, 23: 43, 24: 44, 25: 45, 26: 46, 27: 47}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46, 21: 47}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47}, 8: {0: 47}, 9: {0: 45, 1: 46, 2: 47}}
reward correction dict entry {0: 20, 1: 21, 2: 22, 3: 23, 4: 24, 5: 25, 6: 26, 7: 27, 8: 28, 9: 29, 10: 30, 11: 31, 12: 32, 13: 33, 14: 34, 15: 35, 16: 36, 17: 37, 18: 38, 19: 39, 20: 40, 21: 41, 22: 42, 23: 43, 24: 44, 25: 45, 26: 46, 27: 47}
info for index 4: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '128.4055', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-30.58548', 'orientationReward': '159.9911', 'otherReward': '0', 'velocityReward': '0', 'step': '27', 'amount_of_steps': '28', 'amount_of_steps_based_on_rewardlist': '28', 'bootstrapped_rewards': [1.99331391, 0.401157022, 0.3079775, 1.31596935, 1.96704507, 1.39248312, 2.16881657, 2.08224058, 1.50266516, 1.07471013, 0.7110325, 0.7656615, 0.964872658, 1.16481614, 1.35400331, 1.61615348, 1.61059582, 1.66501176, 1.05791676, 0.8943317, 0.817319155, 0.868875265, 0.8226585, 0.7506264, 0.7003542, 0.367943078, -0.0165951457, 0.0], 'episode': {'r': 128.405585, 'l': 28, 't': 40.49632}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 174, 174, ..., 183, 183, 183],
        [174, 174, 174, ..., 183, 183, 183],
        [173, 173, 174, ..., 182, 182, 183]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 174, 174, ..., 183, 183, 183],
        [174, 174, 174, ..., 183, 183, 183],
        [173, 173, 174, ..., 182, 182, 183]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 174, 174, ..., 183, 183, 183],
        [174, 174, 174, ..., 183, 183, 183],
        [174, 174, 174, ..., 182, 182, 183]]], dtype=uint8)}
insertpos: 48
insertpos: 49
reward correction dict: {0: {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44, 6: 45, 7: 46, 8: 47, 9: 48, 10: 49}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49}, 3: {0: 41, 1: 42, 2: 43, 3: 44, 4: 45, 5: 46, 6: 47, 7: 48, 8: 49}, 4: {0: 48, 1: 49}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46, 21: 47, 22: 48, 23: 49}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49}, 8: {0: 47, 1: 48, 2: 49}, 9: {0: 45, 1: 46, 2: 47, 3: 48, 4: 49}}
reward correction dict entry {0: 45, 1: 46, 2: 47, 3: 48, 4: 49}
info for index 9: {'endEvent': 'WallHit', 'duration': '2.719998', 'cumreward': '3.445932', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-11.04764', 'orientationReward': '15.49341', 'otherReward': '-1', 'velocityReward': '0.0001529263', 'step': '4', 'amount_of_steps': '5', 'amount_of_steps_based_on_rewardlist': '5', 'bootstrapped_rewards': [4.163489, 1.54150951, -0.6311503, -1.20368552, 0.0], 'episode': {'r': 3.445933, 'l': 5, 't': 42.094244}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [121, 122, 122, ..., 127, 127, 127],
        [121, 121, 121, ..., 126, 126, 126],
        [121, 121, 121, ..., 126, 126, 126]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [122, 122, 122, ..., 127, 127, 127],
        [122, 122, 122, ..., 127, 127, 127],
        [122, 122, 122, ..., 126, 127, 127]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [124, 124, 124, ..., 131, 131, 131],
        [124, 123, 124, ..., 130, 130, 130],
        [123, 123, 123, ..., 130, 130, 130]]], dtype=uint8)}
insertpos: 50
insertpos: 51
reward correction dict: {0: {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44, 6: 45, 7: 46, 8: 47, 9: 48, 10: 49, 11: 50, 12: 51}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51}, 3: {0: 41, 1: 42, 2: 43, 3: 44, 4: 45, 5: 46, 6: 47, 7: 48, 8: 49, 9: 50, 10: 51}, 4: {0: 48, 1: 49, 2: 50, 3: 51}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46, 21: 47, 22: 48, 23: 49, 24: 50, 25: 51}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51}, 8: {0: 47, 1: 48, 2: 49, 3: 50, 4: 51}, 9: {0: 50, 1: 51}}
reward correction dict entry {0: 39, 1: 40, 2: 41, 3: 42, 4: 43, 5: 44, 6: 45, 7: 46, 8: 47, 9: 48, 10: 49, 11: 50, 12: 51}
info for index 0: {'endEvent': 'WallHit', 'duration': '8.580007', 'cumreward': '9.657827', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-47.93367', 'orientationReward': '58.59152', 'otherReward': '-1', 'velocityReward': '0', 'step': '12', 'amount_of_steps': '13', 'amount_of_steps_based_on_rewardlist': '13', 'bootstrapped_rewards': [0.9796239, -1.39544213, 0.796545148, -0.160326019, -0.7295796, -0.692217052, -0.428086221, 1.15951288, 0.944060147, 1.48109078, 1.49195135, -0.051517792, 0.0], 'episode': {'r': 9.657828, 'l': 13, 't': 43.47528}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [152, 152, 152, ..., 163, 163, 163],
        [152, 152, 152, ..., 162, 163, 163],
        [152, 152, 152, ..., 162, 162, 162]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [152, 152, 152, ..., 163, 163, 163],
        [152, 152, 152, ..., 162, 163, 163],
        [152, 152, 152, ..., 162, 162, 162]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [153, 153, 153, ..., 164, 164, 164],
        [153, 153, 153, ..., 163, 164, 164],
        [153, 153, 153, ..., 163, 163, 163]]], dtype=uint8)}
insertpos: 52
reward correction dict: {0: {0: 52}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52}, 3: {0: 41, 1: 42, 2: 43, 3: 44, 4: 45, 5: 46, 6: 47, 7: 48, 8: 49, 9: 50, 10: 51, 11: 52}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46, 21: 47, 22: 48, 23: 49, 24: 50, 25: 51, 26: 52}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52}, 8: {0: 47, 1: 48, 2: 49, 3: 50, 4: 51, 5: 52}, 9: {0: 50, 1: 51, 2: 52}}
reward correction dict entry {0: 41, 1: 42, 2: 43, 3: 44, 4: 45, 5: 46, 6: 47, 7: 48, 8: 49, 9: 50, 10: 51, 11: 52}
info for index 3: {'endEvent': 'WallHit', 'duration': '7.839993', 'cumreward': '28.28921', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-29.1218', 'orientationReward': '58.411', 'otherReward': '-1', 'velocityReward': '0', 'step': '11', 'amount_of_steps': '12', 'amount_of_steps_based_on_rewardlist': '12', 'bootstrapped_rewards': [2.43257356, -0.141482815, -0.3903917, -0.5166229, 2.31063962, 3.00791955, 3.27694273, 2.465224, 2.05015564, 1.1182785, -0.132727221, 0.0], 'episode': {'r': 28.289215, 'l': 12, 't': 44.343991}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 172, 120],
        [249, 249, 249, ...,  43,  43,  43],
        [249, 249, 249, ...,  43,  43,  42],
        ...,
        [174, 174, 174, ..., 179, 179, 179],
        [174, 174, 174, ..., 179, 179, 179],
        [174, 174, 174, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 172,  69],
        [249, 249, 249, ...,  43,  43,  43],
        [249, 249, 249, ...,  43,  42,  42],
        ...,
        [174, 174, 174, ..., 179, 179, 179],
        [174, 174, 174, ..., 179, 179, 179],
        [174, 174, 174, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 120,  43,  42],
        [249, 249, 249, ...,  42,  42,  42],
        [249, 249, 249, ...,  42,  42,  42],
        ...,
        [175, 175, 175, ..., 179, 179, 179],
        [175, 175, 175, ..., 179, 179, 179],
        [174, 174, 174, ..., 179, 179, 179]]], dtype=uint8)}
insertpos: 53
reward correction dict: {0: {0: 52, 1: 53}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53}, 3: {0: 53}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53}, 6: {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46, 21: 47, 22: 48, 23: 49, 24: 50, 25: 51, 26: 52, 27: 53}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53}, 8: {0: 47, 1: 48, 2: 49, 3: 50, 4: 51, 5: 52, 6: 53}, 9: {0: 50, 1: 51, 2: 52, 3: 53}}
reward correction dict entry {0: 26, 1: 27, 2: 28, 3: 29, 4: 30, 5: 31, 6: 32, 7: 33, 8: 34, 9: 35, 10: 36, 11: 37, 12: 38, 13: 39, 14: 40, 15: 41, 16: 42, 17: 43, 18: 44, 19: 45, 20: 46, 21: 47, 22: 48, 23: 49, 24: 50, 25: 51, 26: 52, 27: 53}
info for index 6: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '104.2153', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-83.77457', 'orientationReward': '188.9888', 'otherReward': '0', 'velocityReward': '0.0009587528', 'step': '27', 'amount_of_steps': '28', 'amount_of_steps_based_on_rewardlist': '28', 'bootstrapped_rewards': [0.8080307, -1.0606612, -1.68145871, -1.51897073, -1.1746825, -0.8005834, -0.5081142, -0.237523928, 1.67105126, 1.19310117, 0.5155034, 0.4349684, 1.04339588, 1.04704773, 1.28383243, 1.30943048, 1.33355391, 1.07866311, 1.31078446, 1.20556748, 1.33657217, 0.9472274, 0.702778637, 0.6943044, 0.735259235, 0.594006658, 0.0192464367, 0.0], 'episode': {'r': 104.215257, 'l': 28, 't': 45.224201}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [124, 124, 124, ..., 138, 139, 139],
        [124, 124, 124, ..., 138, 138, 138],
        [124, 124, 124, ..., 138, 138, 138]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [124, 124, 124, ..., 138, 139, 139],
        [124, 124, 124, ..., 138, 138, 138],
        [124, 124, 124, ..., 138, 138, 138]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [124, 124, 124, ..., 139, 139, 139],
        [124, 124, 124, ..., 138, 138, 138],
        [124, 124, 124, ..., 138, 138, 138]]], dtype=uint8)}
insertpos: 54
reward correction dict: {0: {0: 52, 1: 53, 2: 54}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53, 26: 54}, 2: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53, 26: 54}, 3: {0: 53, 1: 54}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54}, 6: {0: 54}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54}, 8: {0: 47, 1: 48, 2: 49, 3: 50, 4: 51, 5: 52, 6: 53, 7: 54}, 9: {0: 50, 1: 51, 2: 52, 3: 53, 4: 54}}
reward correction dict entry {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53, 26: 54}
info for index 2: {'endEvent': 'WallHit', 'duration': '19.18025', 'cumreward': '73.49836', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-68.60789', 'orientationReward': '143.1061', 'otherReward': '-1', 'velocityReward': '0', 'step': '26', 'amount_of_steps': '27', 'amount_of_steps_based_on_rewardlist': '27', 'bootstrapped_rewards': [4.160218, 2.13614583, 2.71081471, 2.79700756, 1.45238888, 0.257796228, -0.8010225, -0.84406203, 0.7845451, 1.51005638, 1.674364, 0.6965383, -0.0009932796, -0.00464583933, 0.007609835, 0.0517356, 0.8994494, 1.0258956, 0.7372848, 0.383525521, 0.5227461, 0.7338409, 0.534648, 0.481039077, 0.500442863, 0.03621746, 0.0], 'episode': {'r': 73.498312, 'l': 27, 't': 45.944699}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [156, 156, 156, ..., 163, 164, 164],
        [156, 156, 156, ..., 163, 163, 163],
        [156, 156, 156, ..., 163, 163, 163]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [156, 156, 156, ..., 163, 164, 164],
        [156, 156, 156, ..., 163, 163, 163],
        [156, 156, 156, ..., 163, 163, 163]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [156, 156, 156, ..., 163, 164, 164],
        [156, 156, 156, ..., 163, 163, 163],
        [156, 155, 155, ..., 163, 163, 163]]], dtype=uint8)}
reward correction dict: {0: {0: 52, 1: 53, 2: 54}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53, 26: 54}, 2: {}, 3: {0: 53, 1: 54}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54}, 6: {0: 54}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54}, 8: {0: 47, 1: 48, 2: 49, 3: 50, 4: 51, 5: 52, 6: 53, 7: 54}, 9: {0: 50, 1: 51, 2: 52, 3: 53, 4: 54}}
reward correction dict entry {0: 47, 1: 48, 2: 49, 3: 50, 4: 51, 5: 52, 6: 53, 7: 54}
info for index 8: {'endEvent': 'WallHit', 'duration': '5.219996', 'cumreward': '17.825', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-23.95038', 'orientationReward': '42.77539', 'otherReward': '-1', 'velocityReward': '0', 'step': '7', 'amount_of_steps': '8', 'amount_of_steps_based_on_rewardlist': '8', 'bootstrapped_rewards': [3.09678268, 4.930253, 2.510793, 0.5020347, 0.229948461, 2.322019, 0.6265239, 0.0], 'episode': {'r': 17.825002, 'l': 8, 't': 46.093191}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [173, 173, 173, ..., 178, 178, 178],
        [172, 172, 172, ..., 178, 178, 178],
        [172, 172, 172, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [173, 173, 173, ..., 178, 178, 178],
        [173, 173, 173, ..., 178, 178, 178],
        [172, 173, 172, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [174, 174, 174, ..., 179, 179, 179],
        [174, 174, 174, ..., 178, 178, 178],
        [173, 173, 173, ..., 178, 178, 178]]], dtype=uint8)}
insertpos: 55
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55}, 1: {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53, 26: 54, 27: 55}, 2: {0: 55}, 3: {0: 53, 1: 54, 2: 55}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55}, 6: {0: 54, 1: 55}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55}, 8: {0: 55}, 9: {0: 50, 1: 51, 2: 52, 3: 53, 4: 54, 5: 55}}
reward correction dict entry {0: 28, 1: 29, 2: 30, 3: 31, 4: 32, 5: 33, 6: 34, 7: 35, 8: 36, 9: 37, 10: 38, 11: 39, 12: 40, 13: 41, 14: 42, 15: 43, 16: 44, 17: 45, 18: 46, 19: 47, 20: 48, 21: 49, 22: 50, 23: 51, 24: 52, 25: 53, 26: 54, 27: 55}
info for index 1: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '80.83929', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-33.00308', 'orientationReward': '114.8424', 'otherReward': '0', 'velocityReward': '0', 'step': '27', 'amount_of_steps': '28', 'amount_of_steps_based_on_rewardlist': '28', 'bootstrapped_rewards': [0.7343617, 2.51874852, 0.08991562, 1.91294813, 0.184061155, -1.15929782, -1.27459848, 0.8997891, 1.0457325, 1.07538819, 1.201867, 0.7891267, 0.7270275, 0.782765, 0.4253971, 0.795178354, 0.659446836, 0.8835463, 0.8112777, 0.476140559, 0.715842962, 0.544484258, 0.723703265, 0.578110039, 0.500404954, 0.390294, -0.00021176826, 0.0], 'episode': {'r': 80.839307, 'l': 28, 't': 46.760109}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [177, 177, 177, ..., 179, 179, 179],
        [176, 176, 176, ..., 178, 178, 179],
        [176, 176, 176, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [177, 177, 177, ..., 179, 179, 179],
        [176, 176, 176, ..., 178, 178, 179],
        [176, 176, 176, ..., 178, 178, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [178, 177, 177, ..., 179, 179, 179],
        [177, 177, 177, ..., 178, 178, 178],
        [177, 176, 176, ..., 178, 178, 178]]], dtype=uint8)}
insertpos: 56
insertpos: 57
insertpos: 58
insertpos: 59
insertpos: 60
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60}, 1: {0: 56, 1: 57, 2: 58, 3: 59, 4: 60}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60}, 3: {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60}, 6: {0: 54, 1: 55, 2: 56, 3: 57, 4: 58, 5: 59, 6: 60}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55, 18: 56, 19: 57, 20: 58, 21: 59, 22: 60}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60}, 9: {0: 50, 1: 51, 2: 52, 3: 53, 4: 54, 5: 55, 6: 56, 7: 57, 8: 58, 9: 59, 10: 60}}
reward correction dict entry {0: 50, 1: 51, 2: 52, 3: 53, 4: 54, 5: 55, 6: 56, 7: 57, 8: 58, 9: 59, 10: 60}
info for index 9: {'endEvent': 'WallHit', 'duration': '7.439994', 'cumreward': '12.04452', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-34.46524', 'orientationReward': '47.50948', 'otherReward': '-1', 'velocityReward': '0.000282564', 'step': '10', 'amount_of_steps': '11', 'amount_of_steps_based_on_rewardlist': '11', 'bootstrapped_rewards': [4.256097, 1.81551254, 0.590361834, 1.27670932, -0.343379736, 1.22807038, 1.34943509, 0.2166077, -0.08040095, -0.414788455, 0.0], 'episode': {'r': 12.04453, 'l': 11, 't': 50.5963}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [130, 130, 129, ..., 134, 134, 134],
        [129, 129, 129, ..., 133, 133, 133],
        [129, 129, 129, ..., 133, 133, 133]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [130, 130, 130, ..., 134, 134, 134],
        [130, 130, 130, ..., 134, 134, 134],
        [130, 130, 129, ..., 133, 134, 134]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [132, 132, 132, ..., 136, 136, 136],
        [132, 132, 132, ..., 135, 136, 136],
        [131, 131, 131, ..., 135, 135, 135]]], dtype=uint8)}
insertpos: 61
insertpos: 62
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62}, 1: {0: 56, 1: 57, 2: 58, 3: 59, 4: 60, 5: 61, 6: 62}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62}, 3: {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60, 8: 61, 9: 62}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62}, 6: {0: 54, 1: 55, 2: 56, 3: 57, 4: 58, 5: 59, 6: 60, 7: 61, 8: 62}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55, 18: 56, 19: 57, 20: 58, 21: 59, 22: 60, 23: 61, 24: 62}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62}, 9: {0: 61, 1: 62}}
reward correction dict entry {0: 54, 1: 55, 2: 56, 3: 57, 4: 58, 5: 59, 6: 60, 7: 61, 8: 62}
info for index 6: {'endEvent': 'WallHit', 'duration': '5.559996', 'cumreward': '25.20128', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-9.062643', 'orientationReward': '35.26348', 'otherReward': '-1', 'velocityReward': '0.0004497967', 'step': '8', 'amount_of_steps': '9', 'amount_of_steps_based_on_rewardlist': '9', 'bootstrapped_rewards': [5.151917, 3.87860417, 2.84403, 2.28781462, 2.635726, 1.41076827, 1.35576236, 0.0635352, 0.0], 'episode': {'r': 25.201272, 'l': 9, 't': 52.067525}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [186, 186, 186, ..., 184, 184, 184],
        [186, 186, 186, ..., 184, 184, 184],
        [186, 186, 185, ..., 184, 184, 184]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [187, 186, 186, ..., 184, 184, 185],
        [186, 186, 186, ..., 184, 184, 184],
        [186, 186, 186, ..., 184, 184, 184]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [187, 187, 187, ..., 185, 185, 185],
        [187, 187, 187, ..., 184, 184, 185],
        [186, 186, 186, ..., 184, 184, 184]]], dtype=uint8)}
insertpos: 63
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63}, 1: {0: 56, 1: 57, 2: 58, 3: 59, 4: 60, 5: 61, 6: 62, 7: 63}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63}, 3: {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60, 8: 61, 9: 62, 10: 63}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62, 23: 63}, 6: {0: 63}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55, 18: 56, 19: 57, 20: 58, 21: 59, 22: 60, 23: 61, 24: 62, 25: 63}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63}, 9: {0: 61, 1: 62, 2: 63}}
reward correction dict entry {0: 61, 1: 62, 2: 63}
info for index 9: {'endEvent': 'WallHit', 'duration': '1.459999', 'cumreward': '7.844571', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-0.2209425', 'orientationReward': '9.065109', 'otherReward': '-1', 'velocityReward': '0.0004042416', 'step': '2', 'amount_of_steps': '3', 'amount_of_steps_based_on_rewardlist': '3', 'bootstrapped_rewards': [4.71775341, 2.81413579, 0.0], 'episode': {'r': 7.844571, 'l': 3, 't': 52.896338}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [119, 119, 119, ..., 119, 119, 119],
        [119, 119, 119, ..., 118, 118, 119],
        [119, 119, 119, ..., 118, 118, 118]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [119, 119, 119, ..., 119, 119, 119],
        [119, 119, 119, ..., 119, 119, 119],
        [119, 119, 119, ..., 118, 118, 118]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [119, 119, 119, ..., 119, 119, 119],
        [119, 119, 119, ..., 119, 119, 119],
        [119, 119, 119, ..., 118, 119, 119]]], dtype=uint8)}
insertpos: 64
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63, 12: 64}, 1: {0: 56, 1: 57, 2: 58, 3: 59, 4: 60, 5: 61, 6: 62, 7: 63, 8: 64}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64}, 3: {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60, 8: 61, 9: 62, 10: 63, 11: 64}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62, 23: 63, 24: 64}, 6: {0: 63, 1: 64}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55, 18: 56, 19: 57, 20: 58, 21: 59, 22: 60, 23: 61, 24: 62, 25: 63, 26: 64}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64}, 9: {0: 64}}
reward correction dict entry {0: 56, 1: 57, 2: 58, 3: 59, 4: 60, 5: 61, 6: 62, 7: 63, 8: 64}
info for index 1: {'endEvent': 'WallHit', 'duration': '5.799995', 'cumreward': '21.19938', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-14.37102', 'orientationReward': '36.57033', 'otherReward': '-1', 'velocityReward': '8.46676E-05', 'step': '8', 'amount_of_steps': '9', 'amount_of_steps_based_on_rewardlist': '9', 'bootstrapped_rewards': [3.91401243, 3.02231765, 4.891611, 2.95057273, 0.2809652, 2.07029867, 0.4811518, -0.477770835, 0.0], 'episode': {'r': 21.199371, 'l': 9, 't': 53.540782}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [149, 149, 149, ..., 149, 149, 149],
        [149, 149, 149, ..., 149, 149, 149],
        [149, 148, 148, ..., 149, 149, 149]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [150, 150, 150, ..., 150, 150, 150],
        [150, 150, 150, ..., 150, 150, 150],
        [150, 150, 150, ..., 150, 150, 150]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [153, 153, 153, ..., 152, 152, 152],
        [153, 153, 153, ..., 152, 152, 152],
        [153, 153, 153, ..., 152, 152, 152]]], dtype=uint8)}
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63, 12: 64}, 1: {}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64}, 3: {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60, 8: 61, 9: 62, 10: 63, 11: 64}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62, 23: 63, 24: 64}, 6: {0: 63, 1: 64}, 7: {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55, 18: 56, 19: 57, 20: 58, 21: 59, 22: 60, 23: 61, 24: 62, 25: 63, 26: 64}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64}, 9: {0: 64}}
reward correction dict entry {0: 38, 1: 39, 2: 40, 3: 41, 4: 42, 5: 43, 6: 44, 7: 45, 8: 46, 9: 47, 10: 48, 11: 49, 12: 50, 13: 51, 14: 52, 15: 53, 16: 54, 17: 55, 18: 56, 19: 57, 20: 58, 21: 59, 22: 60, 23: 61, 24: 62, 25: 63, 26: 64}
info for index 7: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '110.4582', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-51.50179', 'orientationReward': '162.9599', 'otherReward': '0', 'velocityReward': '0', 'step': '26', 'amount_of_steps': '27', 'amount_of_steps_based_on_rewardlist': '27', 'bootstrapped_rewards': [1.553621, -0.7105757, 1.4067744, 1.73469031, 0.721264839, 1.86618948, 0.9839288, -0.0003626593, 0.9977512, 0.8614041, 0.8397835, 1.2860384, 1.51500309, 1.13832521, 0.89342165, 0.9808589, 1.45705271, 1.02167153, 1.04190421, 0.9524673, 0.6238523, 0.5743992, 0.9217929, 0.722659, 0.4290866, 0.6210858, 0.0], 'episode': {'r': 110.458177, 'l': 27, 't': 53.737452}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [151, 151, 151, ..., 165, 165, 165],
        [151, 151, 151, ..., 164, 164, 165],
        [151, 151, 151, ..., 164, 164, 164]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [150, 150, 150, ..., 163, 163, 164],
        [150, 150, 150, ..., 163, 163, 163],
        [150, 150, 150, ..., 163, 163, 163]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [149, 149, 149, ..., 163, 163, 163],
        [149, 149, 149, ..., 162, 162, 163],
        [149, 149, 149, ..., 162, 162, 162]]], dtype=uint8)}
insertpos: 65
insertpos: 66
insertpos: 67
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63, 12: 64, 13: 65, 14: 66, 15: 67}, 1: {0: 65, 1: 66, 2: 67}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67}, 3: {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60, 8: 61, 9: 62, 10: 63, 11: 64, 12: 65, 13: 66, 14: 67}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64, 17: 65, 18: 66, 19: 67}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62, 23: 63, 24: 64, 25: 65, 26: 66, 27: 67}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67}, 7: {0: 65, 1: 66, 2: 67}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67}, 9: {0: 64, 1: 65, 2: 66, 3: 67}}
reward correction dict entry {0: 53, 1: 54, 2: 55, 3: 56, 4: 57, 5: 58, 6: 59, 7: 60, 8: 61, 9: 62, 10: 63, 11: 64, 12: 65, 13: 66, 14: 67}
info for index 3: {'endEvent': 'WallHit', 'duration': '10.74006', 'cumreward': '27.17705', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-52.35932', 'orientationReward': '80.53631', 'otherReward': '-1', 'velocityReward': '0', 'step': '14', 'amount_of_steps': '15', 'amount_of_steps_based_on_rewardlist': '15', 'bootstrapped_rewards': [1.74592757, -0.464007229, -0.7273386, -0.6314308, 1.79189622, 1.83645475, 1.53668618, 1.45156431, 0.556296647, 0.350792378, 0.370801032, 1.70258176, 1.35906231, 0.6213393, 0.0], 'episode': {'r': 27.177036, 'l': 15, 't': 55.910651}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [185, 185, 185, ..., 189, 189, 189],
        [185, 185, 185, ..., 188, 188, 188],
        [184, 184, 184, ..., 188, 188, 188]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [187, 187, 187, ..., 189, 189, 189],
        [187, 187, 187, ..., 188, 189, 189],
        [186, 186, 186, ..., 188, 188, 188]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [189, 189, 189, ..., 189, 189, 189],
        [189, 188, 188, ..., 189, 189, 189],
        [188, 188, 188, ..., 188, 188, 188]]], dtype=uint8)}
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63, 12: 64, 13: 65, 14: 66, 15: 67}, 1: {0: 65, 1: 66, 2: 67}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67}, 3: {}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64, 17: 65, 18: 66, 19: 67}, 5: {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62, 23: 63, 24: 64, 25: 65, 26: 66, 27: 67}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67}, 7: {0: 65, 1: 66, 2: 67}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67}, 9: {0: 64, 1: 65, 2: 66, 3: 67}}
reward correction dict entry {0: 40, 1: 41, 2: 42, 3: 43, 4: 44, 5: 45, 6: 46, 7: 47, 8: 48, 9: 49, 10: 50, 11: 51, 12: 52, 13: 53, 14: 54, 15: 55, 16: 56, 17: 57, 18: 58, 19: 59, 20: 60, 21: 61, 22: 62, 23: 63, 24: 64, 25: 65, 26: 66, 27: 67}
info for index 5: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '104.6285', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-57.80962', 'orientationReward': '163.438', 'otherReward': '0', 'velocityReward': '0', 'step': '27', 'amount_of_steps': '28', 'amount_of_steps_based_on_rewardlist': '28', 'bootstrapped_rewards': [1.37137961, 0.806732535, 0.4001994, -0.17731218, 0.0391462259, 0.9793397, 1.30503809, 1.59525168, 1.148784, 1.23970687, 1.37333381, 1.92058766, 2.08818841, 1.08386731, 1.10635412, 0.7996102, 0.678041339, 0.603901744, 0.5913243, 0.7541403, 0.6823213, 0.541144431, 0.699042, 0.7273993, 0.7558877, 0.5001984, -0.0526189581, 0.0], 'episode': {'r': 104.62841, 'l': 28, 't': 55.979603}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [157, 156, 156, ..., 161, 161, 161],
        [156, 156, 156, ..., 161, 161, 161],
        [156, 156, 156, ..., 161, 161, 161]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [156, 156, 156, ..., 161, 161, 161],
        [156, 156, 156, ..., 161, 161, 161],
        [156, 156, 156, ..., 161, 161, 161]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [156, 156, 156, ..., 161, 161, 161],
        [156, 156, 156, ..., 161, 161, 161],
        [156, 156, 156, ..., 160, 160, 161]]], dtype=uint8)}
insertpos: 68
reward correction dict: {0: {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63, 12: 64, 13: 65, 14: 66, 15: 67, 16: 68}, 1: {0: 65, 1: 66, 2: 67, 3: 68}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68}, 3: {0: 68}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64, 17: 65, 18: 66, 19: 67, 20: 68}, 5: {0: 68}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68}, 7: {0: 65, 1: 66, 2: 67, 3: 68}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68}, 9: {0: 64, 1: 65, 2: 66, 3: 67, 4: 68}}
reward correction dict entry {0: 52, 1: 53, 2: 54, 3: 55, 4: 56, 5: 57, 6: 58, 7: 59, 8: 60, 9: 61, 10: 62, 11: 63, 12: 64, 13: 65, 14: 66, 15: 67, 16: 68}
info for index 0: {'endEvent': 'WallHit', 'duration': '11.64008', 'cumreward': '21.71754', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-57.59856', 'orientationReward': '80.31615', 'otherReward': '-1', 'velocityReward': '0', 'step': '16', 'amount_of_steps': '17', 'amount_of_steps_based_on_rewardlist': '17', 'bootstrapped_rewards': [2.12940431, 0.304428756, -0.408238322, 0.02372647, 1.65336645, -0.0226937663, 0.8340438, 1.11719966, 0.815046847, 0.266960233, 0.19650276, 0.152522728, 0.9601835, 0.8665481, 0.7888317, -0.114162847, 0.0], 'episode': {'r': 21.717552, 'l': 17, 't': 56.660265}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [155, 155, 155, ..., 163, 163, 163],
        [155, 155, 155, ..., 163, 163, 163],
        [154, 155, 154, ..., 162, 162, 162]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [155, 155, 155, ..., 163, 163, 163],
        [155, 155, 155, ..., 163, 163, 163],
        [155, 155, 155, ..., 162, 162, 163]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [155, 155, 155, ..., 163, 163, 163],
        [155, 155, 155, ..., 163, 163, 163],
        [155, 155, 155, ..., 162, 163, 163]]], dtype=uint8)}
insertpos: 69
reward correction dict: {0: {0: 69}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69}, 3: {0: 68, 1: 69}, 4: {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64, 17: 65, 18: 66, 19: 67, 20: 68, 21: 69}, 5: {0: 68, 1: 69}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69}, 9: {0: 64, 1: 65, 2: 66, 3: 67, 4: 68, 5: 69}}
reward correction dict entry {0: 48, 1: 49, 2: 50, 3: 51, 4: 52, 5: 53, 6: 54, 7: 55, 8: 56, 9: 57, 10: 58, 11: 59, 12: 60, 13: 61, 14: 62, 15: 63, 16: 64, 17: 65, 18: 66, 19: 67, 20: 68, 21: 69}
info for index 4: {'endEvent': 'WallHit', 'duration': '15.88017', 'cumreward': '56.96239', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-53.80414', 'orientationReward': '111.7666', 'otherReward': '-1', 'velocityReward': '0', 'step': '21', 'amount_of_steps': '22', 'amount_of_steps_based_on_rewardlist': '22', 'bootstrapped_rewards': [3.00261855, 2.06888, 1.55215383, 0.332376361, 0.9945161, 3.75286, 1.57006562, -0.0412169173, -0.323661685, -0.05053591, 1.07326174, 1.49390543, 0.878026, 0.806053, 0.7102417, 0.5207783, 0.5077975, 0.6525449, 0.8368869, 0.823502839, 0.1646166, 0.0], 'episode': {'r': 56.962399, 'l': 22, 't': 57.509636}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [151, 151, 151, ..., 165, 165, 165],
        [151, 151, 151, ..., 164, 165, 165],
        [151, 151, 151, ..., 164, 164, 164]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [151, 151, 151, ..., 165, 165, 165],
        [151, 151, 151, ..., 165, 165, 165],
        [151, 151, 151, ..., 164, 164, 164]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [152, 152, 152, ..., 166, 166, 166],
        [151, 152, 152, ..., 166, 166, 166],
        [151, 151, 151, ..., 165, 165, 165]]], dtype=uint8)}
insertpos: 70
insertpos: 71
insertpos: 72
reward correction dict: {0: {0: 69, 1: 70, 2: 71, 3: 72}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72}, 2: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69, 15: 70, 16: 71, 17: 72}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72}, 4: {0: 70, 1: 71, 2: 72}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69, 7: 70, 8: 71, 9: 72}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69, 15: 70, 16: 71, 17: 72}, 9: {0: 64, 1: 65, 2: 66, 3: 67, 4: 68, 5: 69, 6: 70, 7: 71, 8: 72}}
reward correction dict entry {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69, 15: 70, 16: 71, 17: 72}
info for index 2: {'endEvent': 'WallHit', 'duration': '12.24009', 'cumreward': '45.97045', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-27.34273', 'orientationReward': '74.31313', 'otherReward': '-1', 'velocityReward': '0', 'step': '17', 'amount_of_steps': '18', 'amount_of_steps_based_on_rewardlist': '18', 'bootstrapped_rewards': [1.92040527, 3.931854, 3.8672936, 0.7788552, 0.6680624, 0.416755646, -0.511855245, 2.06317687, 1.97143614, 1.74273849, 0.904682338, 0.8150477, 0.855613, 1.14382863, 0.9954005, 0.6609081, -0.110533968, 0.0], 'episode': {'r': 45.970407, 'l': 18, 't': 59.671615}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [183, 183, 183, ..., 187, 187, 187],
        [183, 183, 183, ..., 187, 187, 187],
        [183, 183, 183, ..., 186, 187, 187]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [183, 183, 183, ..., 187, 187, 188],
        [183, 183, 183, ..., 187, 187, 187],
        [183, 183, 183, ..., 187, 187, 187]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [184, 184, 184, ..., 188, 188, 188],
        [183, 183, 183, ..., 187, 188, 188],
        [183, 183, 183, ..., 187, 187, 187]]], dtype=uint8)}
insertpos: 73
reward correction dict: {0: {0: 69, 1: 70, 2: 71, 3: 72, 4: 73}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73}, 2: {0: 73}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73}, 4: {0: 70, 1: 71, 2: 72, 3: 73}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69, 7: 70, 8: 71, 9: 72, 10: 73}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69, 15: 70, 16: 71, 17: 72, 18: 73}, 9: {0: 64, 1: 65, 2: 66, 3: 67, 4: 68, 5: 69, 6: 70, 7: 71, 8: 72, 9: 73}}
reward correction dict entry {0: 64, 1: 65, 2: 66, 3: 67, 4: 68, 5: 69, 6: 70, 7: 71, 8: 72, 9: 73}
info for index 9: {'endEvent': 'WallHit', 'duration': '6.579995', 'cumreward': '-3.647867', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-38.35925', 'orientationReward': '35.7114', 'otherReward': '-1', 'velocityReward': '0', 'step': '9', 'amount_of_steps': '10', 'amount_of_steps_based_on_rewardlist': '10', 'bootstrapped_rewards': [1.14041269, -0.460158646, -1.24125421, -1.25999367, -0.7958886, -0.28900823, 0.199915856, 0.375351638, -0.204315335, 0.0], 'episode': {'r': -3.647864, 'l': 10, 't': 60.547718}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [145, 145, 145, ..., 145, 145, 145],
        [145, 145, 145, ..., 144, 144, 145],
        [144, 144, 144, ..., 144, 144, 144]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [146, 146, 146, ..., 146, 146, 146],
        [146, 146, 146, ..., 145, 146, 146],
        [146, 145, 145, ..., 145, 145, 145]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [148, 148, 148, ..., 148, 148, 148],
        [148, 148, 148, ..., 147, 147, 147],
        [147, 147, 147, ..., 147, 147, 147]]], dtype=uint8)}
insertpos: 74
insertpos: 75
insertpos: 76
reward correction dict: {0: {0: 69, 1: 70, 2: 71, 3: 72, 4: 73, 5: 74, 6: 75, 7: 76}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76}, 2: {0: 73, 1: 74, 2: 75, 3: 76}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76}, 4: {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69, 7: 70, 8: 71, 9: 72, 10: 73, 11: 74, 12: 75, 13: 76}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76}, 8: {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69, 15: 70, 16: 71, 17: 72, 18: 73, 19: 74, 20: 75, 21: 76}, 9: {0: 74, 1: 75, 2: 76}}
reward correction dict entry {0: 55, 1: 56, 2: 57, 3: 58, 4: 59, 5: 60, 6: 61, 7: 62, 8: 63, 9: 64, 10: 65, 11: 66, 12: 67, 13: 68, 14: 69, 15: 70, 16: 71, 17: 72, 18: 73, 19: 74, 20: 75, 21: 76}
info for index 8: {'endEvent': 'WallHit', 'duration': '15.84017', 'cumreward': '37.95205', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-61.19926', 'orientationReward': '100.1506', 'otherReward': '-1', 'velocityReward': '0.0006831149', 'step': '21', 'amount_of_steps': '22', 'amount_of_steps_based_on_rewardlist': '22', 'bootstrapped_rewards': [2.26855755, 0.341841578, 0.279261231, 0.00353456871, -1.15259528, -1.2784009, 0.828865349, 0.682458937, 1.07882321, 1.52416515, 1.00641418, 1.15286577, 0.45614776, 0.194876239, 0.148612261, 0.14661707, 0.7446481, 0.71683383, 0.463131636, 0.545267642, 0.45061326, 0.0], 'episode': {'r': 37.952054, 'l': 22, 't': 62.781137}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [185, 185, 185, ..., 186, 186, 186],
        [184, 184, 184, ..., 186, 186, 186],
        [184, 184, 184, ..., 185, 185, 185]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [186, 186, 186, ..., 185, 185, 185],
        [185, 185, 185, ..., 184, 184, 185],
        [185, 185, 185, ..., 184, 184, 184]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [187, 186, 186, ..., 184, 184, 184],
        [186, 186, 186, ..., 183, 183, 183],
        [186, 186, 186, ..., 183, 183, 183]]], dtype=uint8)}
insertpos: 77
reward correction dict: {0: {0: 69, 1: 70, 2: 71, 3: 72, 4: 73, 5: 74, 6: 75, 7: 76, 8: 77}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77}, 4: {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76, 7: 77}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69, 7: 70, 8: 71, 9: 72, 10: 73, 11: 74, 12: 75, 13: 76, 14: 77}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77}, 8: {0: 77}, 9: {0: 74, 1: 75, 2: 76, 3: 77}}
reward correction dict entry {0: 69, 1: 70, 2: 71, 3: 72, 4: 73, 5: 74, 6: 75, 7: 76, 8: 77}
info for index 0: {'endEvent': 'WallHit', 'duration': '5.419996', 'cumreward': '21.51816', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-12.86947', 'orientationReward': '35.38761', 'otherReward': '-1', 'velocityReward': '0', 'step': '8', 'amount_of_steps': '9', 'amount_of_steps_based_on_rewardlist': '9', 'bootstrapped_rewards': [3.058559, 1.51356387, 4.15463829, 3.26000452, 2.79011655, 1.16954935, 0.4772717, 0.02175859, 0.0], 'episode': {'r': 21.51815, 'l': 9, 't': 63.422301}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [162, 162, 162, ..., 164, 165, 165],
        [162, 162, 161, ..., 164, 164, 164],
        [161, 161, 161, ..., 164, 164, 164]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [162, 162, 162, ..., 165, 165, 165],
        [162, 162, 162, ..., 164, 164, 164],
        [161, 161, 161, ..., 164, 164, 164]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [163, 163, 163, ..., 165, 165, 165],
        [163, 162, 162, ..., 164, 164, 164],
        [162, 162, 162, ..., 164, 164, 164]]], dtype=uint8)}
insertpos: 78
insertpos: 79
insertpos: 80
reward correction dict: {0: {0: 78, 1: 79, 2: 80}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80}, 4: {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76, 7: 77, 8: 78, 9: 79, 10: 80}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80}, 6: {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69, 7: 70, 8: 71, 9: 72, 10: 73, 11: 74, 12: 75, 13: 76, 14: 77, 15: 78, 16: 79, 17: 80}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80}, 8: {0: 77, 1: 78, 2: 79, 3: 80}, 9: {0: 74, 1: 75, 2: 76, 3: 77, 4: 78, 5: 79, 6: 80}}
reward correction dict entry {0: 63, 1: 64, 2: 65, 3: 66, 4: 67, 5: 68, 6: 69, 7: 70, 8: 71, 9: 72, 10: 73, 11: 74, 12: 75, 13: 76, 14: 77, 15: 78, 16: 79, 17: 80}
info for index 6: {'endEvent': 'WallHit', 'duration': '12.5801', 'cumreward': '58.56007', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-18.39668', 'orientationReward': '77.95381', 'otherReward': '-1', 'velocityReward': '0.002952123', 'step': '17', 'amount_of_steps': '18', 'amount_of_steps_based_on_rewardlist': '18', 'bootstrapped_rewards': [5.0845685, 3.21775627, 2.51637483, 2.28435755, 3.41953826, 2.74467087, 2.80218816, 0.826273859, 2.23525119, 1.67475629, 1.78625166, 1.213152, 1.26790452, 0.503932655, 0.202982739, 0.03826627, 0.13906607, 0.0], 'episode': {'r': 58.560117, 'l': 18, 't': 65.784349}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [162, 162, 162, ..., 171, 171, 171],
        [161, 161, 161, ..., 171, 171, 171],
        [161, 161, 161, ..., 170, 170, 170]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [161, 162, 162, ..., 171, 171, 171],
        [161, 161, 161, ..., 171, 171, 171],
        [161, 161, 161, ..., 170, 170, 170]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [162, 162, 162, ..., 171, 171, 171],
        [162, 162, 162, ..., 171, 171, 171],
        [162, 162, 162, ..., 171, 171, 171]]], dtype=uint8)}
reward correction dict: {0: {0: 78, 1: 79, 2: 80}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80}, 4: {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76, 7: 77, 8: 78, 9: 79, 10: 80}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80}, 6: {}, 7: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80}, 8: {0: 77, 1: 78, 2: 79, 3: 80}, 9: {0: 74, 1: 75, 2: 76, 3: 77, 4: 78, 5: 79, 6: 80}}
reward correction dict entry {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80}
info for index 7: {'endEvent': 'WallHit', 'duration': '10.62005', 'cumreward': '27.11758', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-49.74817', 'orientationReward': '77.86566', 'otherReward': '-1', 'velocityReward': '9.087722E-05', 'step': '15', 'amount_of_steps': '16', 'amount_of_steps_based_on_rewardlist': '16', 'bootstrapped_rewards': [3.38058424, 2.08572268, 0.615929246, -0.277507067, -0.4112897, -0.315638185, -0.164996073, 0.9524134, 1.18006527, 1.40246928, 1.65695357, 0.880782843, 0.497534931, 1.30821109, -0.0704812258, 0.0], 'episode': {'r': 27.117582, 'l': 16, 't': 65.838142}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [165, 165, 165, ..., 173, 173, 174],
        [165, 165, 165, ..., 173, 173, 173],
        [165, 165, 165, ..., 172, 173, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [165, 165, 165, ..., 174, 174, 174],
        [165, 165, 165, ..., 173, 173, 173],
        [165, 165, 165, ..., 172, 173, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [165, 165, 165, ..., 173, 174, 174],
        [165, 165, 165, ..., 173, 173, 173],
        [165, 165, 165, ..., 173, 173, 173]]], dtype=uint8)}
insertpos: 81
insertpos: 82
insertpos: 83
insertpos: 84
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80, 16: 81, 17: 82, 18: 83, 19: 84}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84}, 4: {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76, 7: 77, 8: 78, 9: 79, 10: 80, 11: 81, 12: 82, 13: 83, 14: 84}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84}, 6: {0: 81, 1: 82, 2: 83, 3: 84}, 7: {0: 81, 1: 82, 2: 83, 3: 84}, 8: {0: 77, 1: 78, 2: 79, 3: 80, 4: 81, 5: 82, 6: 83, 7: 84}, 9: {0: 74, 1: 75, 2: 76, 3: 77, 4: 78, 5: 79, 6: 80, 7: 81, 8: 82, 9: 83, 10: 84}}
reward correction dict entry {0: 74, 1: 75, 2: 76, 3: 77, 4: 78, 5: 79, 6: 80, 7: 81, 8: 82, 9: 83, 10: 84}
info for index 9: {'endEvent': 'WallHit', 'duration': '6.839994', 'cumreward': '2.775135', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-39.79107', 'orientationReward': '43.56623', 'otherReward': '-1', 'velocityReward': '0', 'step': '10', 'amount_of_steps': '11', 'amount_of_steps_based_on_rewardlist': '11', 'bootstrapped_rewards': [1.40452027, -0.2364386, -0.341949373, 0.065732494, -0.0252335556, 0.425272644, 0.339102924, 0.34539935, 0.385787576, -0.3769057, 0.0], 'episode': {'r': 2.775136, 'l': 11, 't': 68.871051}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 224],
        [249, 249, 249, ..., 146,  69,  44],
        [249, 249, 249, ...,  43,  43,  43],
        ...,
        [149, 149, 149, ..., 144, 143, 144],
        [149, 149, 149, ..., 143, 143, 143],
        [148, 148, 148, ..., 143, 143, 143]],

       [[249, 249, 249, ..., 249, 249, 224],
        [249, 249, 249, ..., 146,  69,  44],
        [249, 249, 249, ...,  43,  43,  43],
        ...,
        [149, 149, 149, ..., 143, 143, 144],
        [149, 149, 149, ..., 143, 143, 143],
        [148, 148, 148, ..., 143, 143, 143]],

       [[249, 249, 249, ..., 249, 249, 172],
        [249, 249, 249, ...,  95,  44,  43],
        [249, 249, 249, ...,  43,  43,  42],
        ...,
        [149, 149, 149, ..., 144, 144, 144],
        [149, 149, 149, ..., 143, 143, 143],
        [148, 148, 148, ..., 143, 143, 143]]], dtype=uint8)}
insertpos: 85
insertpos: 86
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80, 16: 81, 17: 82, 18: 83, 19: 84, 20: 85, 21: 86}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84, 12: 85, 13: 86}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86}, 4: {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76, 7: 77, 8: 78, 9: 79, 10: 80, 11: 81, 12: 82, 13: 83, 14: 84, 15: 85, 16: 86}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86}, 8: {0: 77, 1: 78, 2: 79, 3: 80, 4: 81, 5: 82, 6: 83, 7: 84, 8: 85, 9: 86}, 9: {0: 85, 1: 86}}
reward correction dict entry {0: 70, 1: 71, 2: 72, 3: 73, 4: 74, 5: 75, 6: 76, 7: 77, 8: 78, 9: 79, 10: 80, 11: 81, 12: 82, 13: 83, 14: 84, 15: 85, 16: 86}
info for index 4: {'endEvent': 'WallHit', 'duration': '11.70008', 'cumreward': '35.06094', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-43.28368', 'orientationReward': '79.34457', 'otherReward': '-1', 'velocityReward': '0', 'step': '16', 'amount_of_steps': '17', 'amount_of_steps_based_on_rewardlist': '17', 'bootstrapped_rewards': [1.50940859, 0.932251155, 1.10978389, 0.207391724, -0.126621, 0.7881723, 1.00318849, 0.225484133, 0.09577735, 0.8669009, 1.747864, 1.35953, 1.26005352, 1.36700237, 1.05138874, 0.230520725, 0.0], 'episode': {'r': 35.060936, 'l': 17, 't': 70.337142}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [158, 158, 158, ..., 169, 169, 169],
        [158, 158, 158, ..., 168, 168, 169],
        [158, 158, 158, ..., 168, 168, 168]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [158, 158, 158, ..., 169, 169, 169],
        [158, 158, 158, ..., 168, 168, 169],
        [158, 158, 158, ..., 168, 168, 168]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [158, 158, 158, ..., 171, 171, 171],
        [158, 158, 158, ..., 170, 170, 170],
        [158, 158, 158, ..., 170, 170, 170]]], dtype=uint8)}
insertpos: 87
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86, 9: 87}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80, 16: 81, 17: 82, 18: 83, 19: 84, 20: 85, 21: 86, 22: 87}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84, 12: 85, 13: 86, 14: 87}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87}, 4: {0: 87}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87}, 8: {0: 77, 1: 78, 2: 79, 3: 80, 4: 81, 5: 82, 6: 83, 7: 84, 8: 85, 9: 86, 10: 87}, 9: {0: 85, 1: 86, 2: 87}}
reward correction dict entry {0: 77, 1: 78, 2: 79, 3: 80, 4: 81, 5: 82, 6: 83, 7: 84, 8: 85, 9: 86, 10: 87}
info for index 8: {'endEvent': 'WallHit', 'duration': '7.219994', 'cumreward': '17.0049', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-31.94157', 'orientationReward': '49.9465', 'otherReward': '-1', 'velocityReward': '0', 'step': '10', 'amount_of_steps': '11', 'amount_of_steps_based_on_rewardlist': '11', 'bootstrapped_rewards': [1.89322257, -0.134827435, 3.368848, 3.29632449, 1.67002034, 0.3584484, 0.904791057, 0.8891875, 0.281757921, -0.3052216, 0.0], 'episode': {'r': 17.004909, 'l': 11, 't': 71.212328}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [180, 180, 180, ..., 180, 180, 180],
        [180, 180, 180, ..., 180, 180, 180],
        [180, 179, 179, ..., 180, 180, 180]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [180, 180, 180, ..., 180, 180, 180],
        [180, 180, 180, ..., 180, 180, 180],
        [180, 180, 179, ..., 180, 180, 180]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [182, 182, 182, ..., 181, 182, 182],
        [181, 181, 181, ..., 181, 181, 181],
        [181, 181, 181, ..., 181, 181, 181]]], dtype=uint8)}
insertpos: 88
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86, 9: 87, 10: 88}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80, 16: 81, 17: 82, 18: 83, 19: 84, 20: 85, 21: 86, 22: 87, 23: 88}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84, 12: 85, 13: 86, 14: 87, 15: 88}, 3: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88}, 4: {0: 87, 1: 88}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88}, 8: {0: 88}, 9: {0: 85, 1: 86, 2: 87, 3: 88}}
reward correction dict entry {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88}
info for index 3: {'endEvent': 'WallHit', 'duration': '14.76015', 'cumreward': '57.53324', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-57.78361', 'orientationReward': '116.3168', 'otherReward': '-1', 'velocityReward': '0', 'step': '20', 'amount_of_steps': '21', 'amount_of_steps_based_on_rewardlist': '21', 'bootstrapped_rewards': [1.912034, 2.91488647, 0.9828367, 1.82874763, 1.52098417, 2.5726974, 1.04308009, -0.0138813052, -0.2398146, 1.6747396, 1.16495264, 0.89537406, 0.7450804, 0.424243748, 1.12223089, 1.0310967, 0.780529439, 0.8454067, 0.776886761, 0.19095704, 0.0], 'episode': {'r': 57.533238, 'l': 21, 't': 71.956277}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [171, 171, 171, ..., 178, 178, 178],
        [171, 171, 171, ..., 178, 178, 178],
        [170, 170, 170, ..., 177, 177, 178]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [171, 171, 171, ..., 178, 178, 178],
        [171, 171, 171, ..., 178, 178, 178],
        [170, 170, 170, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [171, 171, 171, ..., 178, 178, 178],
        [171, 171, 171, ..., 178, 178, 178],
        [171, 171, 171, ..., 178, 178, 178]]], dtype=uint8)}
insertpos: 89
insertpos: 90
insertpos: 91
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86, 9: 87, 10: 88, 11: 89, 12: 90, 13: 91}, 1: {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80, 16: 81, 17: 82, 18: 83, 19: 84, 20: 85, 21: 86, 22: 87, 23: 88, 24: 89, 25: 90, 26: 91}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84, 12: 85, 13: 86, 14: 87, 15: 88, 16: 89, 17: 90, 18: 91}, 3: {0: 89, 1: 90, 2: 91}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88, 21: 89, 22: 90, 23: 91}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91}, 8: {0: 88, 1: 89, 2: 90, 3: 91}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91}}
reward correction dict entry {0: 65, 1: 66, 2: 67, 3: 68, 4: 69, 5: 70, 6: 71, 7: 72, 8: 73, 9: 74, 10: 75, 11: 76, 12: 77, 13: 78, 14: 79, 15: 80, 16: 81, 17: 82, 18: 83, 19: 84, 20: 85, 21: 86, 22: 87, 23: 88, 24: 89, 25: 90, 26: 91}
info for index 1: {'endEvent': 'WallHit', 'duration': '19.06025', 'cumreward': '67.56003', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-81.27869', 'orientationReward': '149.8386', 'otherReward': '-1', 'velocityReward': '0', 'step': '26', 'amount_of_steps': '27', 'amount_of_steps_based_on_rewardlist': '27', 'bootstrapped_rewards': [0.7043062, 2.972519, 1.001809, 1.02958655, 1.36745417, 0.7746044, 0.6216034, 1.78185213, 0.641336262, -0.2841826, -0.38467446, -0.348829925, -0.159169585, 0.636242747, 1.00803149, 0.649261832, 0.322125435, 0.273516029, 0.7087417, 0.7610978, 0.55569756, 0.6905519, 0.702255368, 0.595837533, 0.536102355, -0.04446563, 0.0], 'episode': {'r': 67.560026, 'l': 27, 't': 74.197911}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [167, 167, 167, ..., 173, 173, 174],
        [167, 167, 167, ..., 173, 173, 173],
        [167, 166, 166, ..., 172, 172, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [167, 167, 167, ..., 173, 173, 173],
        [167, 167, 167, ..., 173, 173, 173],
        [167, 166, 166, ..., 172, 173, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [167, 167, 167, ..., 173, 173, 174],
        [167, 167, 167, ..., 173, 173, 173],
        [167, 166, 166, ..., 172, 173, 173]]], dtype=uint8)}
insertpos: 92
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86, 9: 87, 10: 88, 11: 89, 12: 90, 13: 91, 14: 92}, 1: {0: 92}, 2: {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84, 12: 85, 13: 86, 14: 87, 15: 88, 16: 89, 17: 90, 18: 91, 19: 92}, 3: {0: 89, 1: 90, 2: 91, 3: 92}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88, 21: 89, 22: 90, 23: 91, 24: 92}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92}, 8: {0: 88, 1: 89, 2: 90, 3: 91, 4: 92}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92}}
reward correction dict entry {0: 73, 1: 74, 2: 75, 3: 76, 4: 77, 5: 78, 6: 79, 7: 80, 8: 81, 9: 82, 10: 83, 11: 84, 12: 85, 13: 86, 14: 87, 15: 88, 16: 89, 17: 90, 18: 91, 19: 92}
info for index 2: {'endEvent': 'WallHit', 'duration': '14.36014', 'cumreward': '60.49643', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-16.00651', 'orientationReward': '77.49569', 'otherReward': '-1', 'velocityReward': '0.007189658', 'step': '19', 'amount_of_steps': '20', 'amount_of_steps_based_on_rewardlist': '20', 'bootstrapped_rewards': [2.03050733, 1.37033343, 1.80689371, 0.52487, 1.13557541, 0.809520245, 2.22253633, 2.55098081, 1.42256284, 1.08321083, 1.05049741, 1.82770729, 1.58630633, 1.1898489, 0.9816257, 0.8457712, 0.421315372, 0.6585884, 0.227910236, 0.0], 'episode': {'r': 60.496391, 'l': 20, 't': 75.018468}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [157, 157, 157, ..., 167, 167, 167],
        [157, 157, 157, ..., 166, 167, 167],
        [157, 157, 157, ..., 166, 166, 166]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [157, 157, 156, ..., 167, 167, 167],
        [156, 156, 156, ..., 166, 166, 166],
        [156, 156, 156, ..., 166, 166, 166]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [155, 155, 155, ..., 165, 165, 166],
        [155, 155, 155, ..., 165, 165, 165],
        [155, 155, 155, ..., 165, 165, 165]]], dtype=uint8)}
insertpos: 93
insertpos: 94
reward correction dict: {0: {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86, 9: 87, 10: 88, 11: 89, 12: 90, 13: 91, 14: 92, 15: 93, 16: 94}, 1: {0: 92, 1: 93, 2: 94}, 2: {0: 93, 1: 94}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92, 6: 93, 7: 94}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88, 21: 89, 22: 90, 23: 91, 24: 92, 25: 93, 26: 94}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94}, 8: {0: 88, 1: 89, 2: 90, 3: 91, 4: 92, 5: 93, 6: 94}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94}}
reward correction dict entry {0: 78, 1: 79, 2: 80, 3: 81, 4: 82, 5: 83, 6: 84, 7: 85, 8: 86, 9: 87, 10: 88, 11: 89, 12: 90, 13: 91, 14: 92, 15: 93, 16: 94}
info for index 0: {'endEvent': 'WallHit', 'duration': '11.92008', 'cumreward': '43.09521', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-40.88172', 'orientationReward': '84.97682', 'otherReward': '-1', 'velocityReward': '7.951074E-05', 'step': '16', 'amount_of_steps': '17', 'amount_of_steps_based_on_rewardlist': '17', 'bootstrapped_rewards': [2.6646595, 0.396396726, -0.9680304, -0.968154848, 1.83800328, 2.570123, 1.54475522, 0.356629759, 1.63722038, 1.96192884, 1.56509984, 1.0861969, 1.37899876, 0.9918772, 0.914445341, 0.433916867, 0.0], 'episode': {'r': 43.095213, 'l': 17, 't': 76.501756}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [154, 154, 154, ..., 163, 163, 163],
        [154, 154, 153, ..., 162, 163, 163],
        [153, 153, 153, ..., 162, 162, 162]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [154, 154, 154, ..., 163, 163, 163],
        [154, 153, 153, ..., 162, 163, 163],
        [153, 153, 153, ..., 162, 162, 162]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [153, 153, 153, ..., 163, 163, 163],
        [153, 153, 153, ..., 162, 162, 162],
        [153, 153, 153, ..., 162, 162, 162]]], dtype=uint8)}
insertpos: 95
reward correction dict: {0: {0: 95}, 1: {0: 92, 1: 93, 2: 94, 3: 95}, 2: {0: 93, 1: 94, 2: 95}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94, 6: 95}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92, 6: 93, 7: 94, 8: 95}, 5: {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88, 21: 89, 22: 90, 23: 91, 24: 92, 25: 93, 26: 94, 27: 95}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95}, 8: {0: 88, 1: 89, 2: 90, 3: 91, 4: 92, 5: 93, 6: 94, 7: 95}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94, 10: 95}}
reward correction dict entry {0: 68, 1: 69, 2: 70, 3: 71, 4: 72, 5: 73, 6: 74, 7: 75, 8: 76, 9: 77, 10: 78, 11: 79, 12: 80, 13: 81, 14: 82, 15: 83, 16: 84, 17: 85, 18: 86, 19: 87, 20: 88, 21: 89, 22: 90, 23: 91, 24: 92, 25: 93, 26: 94, 27: 95}
info for index 5: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '91.87439', 'passedGoals': '0', 'numberOfGoals': '1', 'distanceReward': '-49.40952', 'orientationReward': '142.2838', 'otherReward': '0', 'velocityReward': '0', 'step': '27', 'amount_of_steps': '28', 'amount_of_steps_based_on_rewardlist': '28', 'bootstrapped_rewards': [1.7150296, 3.389192, 1.55619562, 2.96162343, 2.192885, 0.5630499, -0.34595716, 0.04106438, 1.10526562, 0.969991148, 0.3991188, 0.723308444, 1.2850399, 1.02775359, 0.9643894, 0.7604228, 0.937115, 0.7600655, 0.638076544, 0.892920554, 0.645476162, 0.447673529, 0.4557896, 0.4367869, 0.4413593, 0.453025818, 0.0245005339, 0.0], 'episode': {'r': 91.874373, 'l': 28, 't': 77.375473}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [180, 180, 180, ..., 187, 188, 188],
        [180, 180, 179, ..., 187, 187, 187],
        [179, 179, 179, ..., 186, 187, 187]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [180, 180, 180, ..., 187, 188, 188],
        [180, 180, 180, ..., 187, 187, 187],
        [179, 179, 179, ..., 186, 187, 187]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [180, 180, 180, ..., 187, 188, 188],
        [180, 180, 180, ..., 187, 187, 187],
        [180, 180, 180, ..., 187, 187, 187]]], dtype=uint8)}
insertpos: 96
insertpos: 97
insertpos: 98
reward correction dict: {0: {0: 95, 1: 96, 2: 97, 3: 98}, 1: {0: 92, 1: 93, 2: 94, 3: 95, 4: 96, 5: 97, 6: 98}, 2: {0: 93, 1: 94, 2: 95, 3: 96, 4: 97, 5: 98}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94, 6: 95, 7: 96, 8: 97, 9: 98}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92, 6: 93, 7: 94, 8: 95, 9: 96, 10: 97, 11: 98}, 5: {0: 96, 1: 97, 2: 98}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98}, 8: {0: 88, 1: 89, 2: 90, 3: 91, 4: 92, 5: 93, 6: 94, 7: 95, 8: 96, 9: 97, 10: 98}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94, 10: 95, 11: 96, 12: 97, 13: 98}}
reward correction dict entry {0: 88, 1: 89, 2: 90, 3: 91, 4: 92, 5: 93, 6: 94, 7: 95, 8: 96, 9: 97, 10: 98}
info for index 8: {'endEvent': 'WallHit', 'duration': '6.999994', 'cumreward': '36.74438', 'passedGoals': '0', 'numberOfGoals': '3', 'distanceReward': '-14.47122', 'orientationReward': '52.2156', 'otherReward': '-1', 'velocityReward': '0', 'step': '10', 'amount_of_steps': '11', 'amount_of_steps_based_on_rewardlist': '11', 'bootstrapped_rewards': [2.83679676, 2.85823774, 3.88756156, 4.16892242, 3.39138174, 3.15211821, 2.91789651, 1.65379751, 0.668460965, -0.307849258, 0.0], 'episode': {'r': 36.744384, 'l': 11, 't': 79.648135}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 178, 178, 178],
        [169, 169, 169, ..., 177, 177, 178],
        [169, 169, 169, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [169, 169, 169, ..., 178, 178, 178],
        [169, 169, 169, ..., 177, 177, 178],
        [169, 169, 169, ..., 177, 177, 177]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [170, 170, 170, ..., 178, 178, 178],
        [169, 170, 169, ..., 177, 177, 178],
        [169, 169, 169, ..., 177, 177, 177]]], dtype=uint8)}
requests.exceptions.ConnectionError => ignoring, retrying
old step mistake step -1, current mistake step 18, env 6
step_mistakes 1
insertpos: 99
reward correction dict: {0: {0: 95, 1: 96, 2: 97, 3: 98, 4: 99}, 1: {0: 92, 1: 93, 2: 94, 3: 95, 4: 96, 5: 97, 6: 98, 7: 99}, 2: {0: 93, 1: 94, 2: 95, 3: 96, 4: 97, 5: 98, 6: 99}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94, 6: 95, 7: 96, 8: 97, 9: 98, 10: 99}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92, 6: 93, 7: 94, 8: 95, 9: 96, 10: 97, 11: 98, 12: 99}, 5: {0: 96, 1: 97, 2: 98, 3: 99}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99}, 8: {0: 99}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94, 10: 95, 11: 96, 12: 97, 13: 98, 14: 99}}
reward correction dict entry {0: 92, 1: 93, 2: 94, 3: 95, 4: 96, 5: 97, 6: 98, 7: 99}
info for index 1: {'endEvent': 'WallHit', 'duration': '5.179996', 'cumreward': '-3.721761', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-27.0183', 'orientationReward': '24.29654', 'otherReward': '-1', 'velocityReward': '0', 'step': '7', 'amount_of_steps': '8', 'amount_of_steps_based_on_rewardlist': '8', 'bootstrapped_rewards': [1.68836236, -1.44538856, 0.8025975, -0.288285017, -1.10699213, -1.10802186, -0.44421497, 0.0], 'episode': {'r': -3.721762, 'l': 8, 't': 80.315658}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [181, 181, 181, ..., 169, 169, 169],
        [180, 180, 180, ..., 168, 168, 168],
        [180, 180, 180, ..., 168, 168, 168]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [181, 181, 181, ..., 169, 169, 169],
        [181, 180, 180, ..., 169, 169, 169],
        [180, 180, 180, ..., 169, 168, 168]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [181, 181, 181, ..., 170, 170, 170],
        [181, 181, 181, ..., 170, 170, 170],
        [180, 180, 180, ..., 169, 169, 169]]], dtype=uint8)}
old step mistake step 18, current mistake step 19, env 6
step_mistakes 2
insertpos: 100
old step mistake step 19, current mistake step 20, env 6
step_mistakes 3
insertpos: 101
old step mistake step 20, current mistake step 21, env 6
step_mistakes 4
insertpos: 102
old step mistake step 21, current mistake step 22, env 6
step_mistakes 5
insertpos: 103
old step mistake step 22, current mistake step 23, env 6
step_mistakes 6
insertpos: 104
old step mistake step 23, current mistake step 24, env 6
step_mistakes 7
insertpos: 105
old step mistake step 24, current mistake step 25, env 6
step_mistakes 8
insertpos: 106
old step mistake step 25, current mistake step 26, env 6
step_mistakes 9
insertpos: 107
reward correction dict: {0: {0: 95, 1: 96, 2: 97, 3: 98, 4: 99, 5: 100, 6: 101, 7: 102, 8: 103, 9: 104, 10: 105, 11: 106, 12: 107}, 1: {0: 100, 1: 101, 2: 102, 3: 103, 4: 104, 5: 105, 6: 106, 7: 107}, 2: {0: 93, 1: 94, 2: 95, 3: 96, 4: 97, 5: 98, 6: 99, 7: 100, 8: 101, 9: 102, 10: 103, 11: 104, 12: 105, 13: 106, 14: 107}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94, 6: 95, 7: 96, 8: 97, 9: 98, 10: 99, 11: 100, 12: 101, 13: 102, 14: 103, 15: 104, 16: 105, 17: 106, 18: 107}, 4: {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92, 6: 93, 7: 94, 8: 95, 9: 96, 10: 97, 11: 98, 12: 99, 13: 100, 14: 101, 15: 102, 16: 103, 17: 104, 18: 105, 19: 106, 20: 107}, 5: {0: 96, 1: 97, 2: 98, 3: 99, 4: 100, 5: 101, 6: 102, 7: 103, 8: 104, 9: 105, 10: 106, 11: 107}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107}, 8: {0: 99, 1: 100, 2: 101, 3: 102, 4: 103, 5: 104, 6: 105, 7: 106, 8: 107}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94, 10: 95, 11: 96, 12: 97, 13: 98, 14: 99, 15: 100, 16: 101, 17: 102, 18: 103, 19: 104, 20: 105, 21: 106, 22: 107}}
reward correction dict entry {0: 87, 1: 88, 2: 89, 3: 90, 4: 91, 5: 92, 6: 93, 7: 94, 8: 95, 9: 96, 10: 97, 11: 98, 12: 99, 13: 100, 14: 101, 15: 102, 16: 103, 17: 104, 18: 105, 19: 106, 20: 107}
info for index 4: {'endEvent': 'WallHit', 'duration': '14.94015', 'cumreward': '57.92906', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-57.56304', 'orientationReward': '116.4921', 'otherReward': '-1', 'velocityReward': '0', 'step': '20', 'amount_of_steps': '21', 'amount_of_steps_based_on_rewardlist': '21', 'bootstrapped_rewards': [1.45083916, 1.444596, -0.152194366, 1.72903, 0.7948722, -0.167794421, 1.40404773, 1.225953, 1.3568759, 1.74701571, 1.29998171, 1.09771335, 1.3064059, 1.086103, 1.09216273, 0.8378402, 0.688134253, 0.5995482, 0.6901853, 0.272885352, 0.0], 'episode': {'r': 57.929042, 'l': 21, 't': 86.475275}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [164, 164, 164, ..., 173, 174, 174],
        [164, 164, 164, ..., 173, 173, 173],
        [164, 164, 164, ..., 172, 173, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [164, 164, 164, ..., 173, 173, 174],
        [164, 164, 164, ..., 173, 173, 173],
        [164, 164, 164, ..., 172, 173, 173]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [164, 164, 164, ..., 174, 174, 174],
        [164, 164, 164, ..., 173, 173, 173],
        [164, 164, 164, ..., 173, 173, 173]]], dtype=uint8)}
old step mistake step 26, current mistake step 27, env 6
step_mistakes 10
insertpos: 108
reward correction dict: {0: {0: 95, 1: 96, 2: 97, 3: 98, 4: 99, 5: 100, 6: 101, 7: 102, 8: 103, 9: 104, 10: 105, 11: 106, 12: 107, 13: 108}, 1: {0: 100, 1: 101, 2: 102, 3: 103, 4: 104, 5: 105, 6: 106, 7: 107, 8: 108}, 2: {0: 93, 1: 94, 2: 95, 3: 96, 4: 97, 5: 98, 6: 99, 7: 100, 8: 101, 9: 102, 10: 103, 11: 104, 12: 105, 13: 106, 14: 107, 15: 108}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94, 6: 95, 7: 96, 8: 97, 9: 98, 10: 99, 11: 100, 12: 101, 13: 102, 14: 103, 15: 104, 16: 105, 17: 106, 18: 107, 19: 108}, 4: {0: 108}, 5: {0: 96, 1: 97, 2: 98, 3: 99, 4: 100, 5: 101, 6: 102, 7: 103, 8: 104, 9: 105, 10: 106, 11: 107, 12: 108}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107, 27: 108}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107, 27: 108}, 8: {0: 99, 1: 100, 2: 101, 3: 102, 4: 103, 5: 104, 6: 105, 7: 106, 8: 107, 9: 108}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94, 10: 95, 11: 96, 12: 97, 13: 98, 14: 99, 15: 100, 16: 101, 17: 102, 18: 103, 19: 104, 20: 105, 21: 106, 22: 107, 23: 108}}
reward correction dict entry {0: 96, 1: 97, 2: 98, 3: 99, 4: 100, 5: 101, 6: 102, 7: 103, 8: 104, 9: 105, 10: 106, 11: 107, 12: 108}
info for index 5: {'endEvent': 'WallHit', 'duration': '8.400002', 'cumreward': '15.07902', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-37.49461', 'orientationReward': '53.57364', 'otherReward': '-1', 'velocityReward': '0', 'step': '12', 'amount_of_steps': '13', 'amount_of_steps_based_on_rewardlist': '13', 'bootstrapped_rewards': [2.22481012, 0.315967232, -0.9835118, 0.9769406, 1.047653, 0.2459855, -0.04413616, 0.5825961, 0.9468014, 1.49752927, 1.38844693, -0.2567065, 0.0], 'episode': {'r': 15.079023, 'l': 13, 't': 87.279484}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [160, 160, 160, ..., 170, 170, 171],
        [160, 160, 160, ..., 170, 170, 170],
        [160, 160, 160, ..., 169, 170, 170]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [160, 160, 160, ..., 170, 170, 171],
        [160, 160, 160, ..., 170, 170, 170],
        [160, 160, 160, ..., 169, 170, 170]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [160, 160, 160, ..., 170, 170, 171],
        [160, 160, 160, ..., 170, 170, 170],
        [160, 160, 160, ..., 169, 169, 170]]], dtype=uint8)}
reward correction dict: {0: {0: 95, 1: 96, 2: 97, 3: 98, 4: 99, 5: 100, 6: 101, 7: 102, 8: 103, 9: 104, 10: 105, 11: 106, 12: 107, 13: 108}, 1: {0: 100, 1: 101, 2: 102, 3: 103, 4: 104, 5: 105, 6: 106, 7: 107, 8: 108}, 2: {0: 93, 1: 94, 2: 95, 3: 96, 4: 97, 5: 98, 6: 99, 7: 100, 8: 101, 9: 102, 10: 103, 11: 104, 12: 105, 13: 106, 14: 107, 15: 108}, 3: {0: 89, 1: 90, 2: 91, 3: 92, 4: 93, 5: 94, 6: 95, 7: 96, 8: 97, 9: 98, 10: 99, 11: 100, 12: 101, 13: 102, 14: 103, 15: 104, 16: 105, 17: 106, 18: 107, 19: 108}, 4: {0: 108}, 5: {}, 6: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107, 27: 108}, 7: {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107, 27: 108}, 8: {0: 99, 1: 100, 2: 101, 3: 102, 4: 103, 5: 104, 6: 105, 7: 106, 8: 107, 9: 108}, 9: {0: 85, 1: 86, 2: 87, 3: 88, 4: 89, 5: 90, 6: 91, 7: 92, 8: 93, 9: 94, 10: 95, 11: 96, 12: 97, 13: 98, 14: 99, 15: 100, 16: 101, 17: 102, 18: 103, 19: 104, 20: 105, 21: 106, 22: 107, 23: 108}}
reward correction dict entry {0: 81, 1: 82, 2: 83, 3: 84, 4: 85, 5: 86, 6: 87, 7: 88, 8: 89, 9: 90, 10: 91, 11: 92, 12: 93, 13: 94, 14: 95, 15: 96, 16: 97, 17: 98, 18: 99, 19: 100, 20: 101, 21: 102, 22: 103, 23: 104, 24: 105, 25: 106, 26: 107, 27: 108}
info for index 6: {'endEvent': 'OutOfTime', 'duration': '20.00027', 'cumreward': '115.3367', 'passedGoals': '0', 'numberOfGoals': '2', 'distanceReward': '-43.5291', 'orientationReward': '159.8639', 'otherReward': '0', 'velocityReward': '0.001949325', 'step': 27, 'amount_of_steps': '29', 'amount_of_steps_based_on_rewardlist': '29', 'bootstrapped_rewards': [2.47672224, 1.48394239, 0.750458956, 1.29053688, 1.43268585, 0.720300853, 0.289438665, 0.758612633, 1.55022407, 1.69162047, 0.9843666, 0.9424788, 1.7418474, 1.21995139, 1.02916241, 1.51436627, 1.42243636, 1.71171987, 0.2640511, 0.872548342, 0.712478638, 0.781684637, 0.487544417, 0.459505826, 0.3732499, 0.3174161, 0.527878761, -0.0118564926, 0.0], 'episode': {'r': 105.072818, 'l': 28, 't': 87.338355}, 'TimeLimit.truncated': False, 'terminal_observation': array([[[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [163, 163, 163, ..., 173, 173, 173],
        [163, 163, 163, ..., 172, 172, 173],
        [163, 163, 163, ..., 172, 172, 172]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [163, 163, 163, ..., 173, 173, 173],
        [163, 163, 163, ..., 172, 172, 173],
        [163, 163, 163, ..., 172, 172, 172]],

       [[249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        [249, 249, 249, ..., 249, 249, 249],
        ...,
        [163, 163, 163, ..., 173, 173, 173],
        [163, 163, 163, ..., 172, 172, 173],
        [163, 163, 163, ..., 172, 172, 172]]], dtype=uint8)}
