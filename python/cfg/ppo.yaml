comment: "differentialJetBot is much better, trying the training with all Maps + OrientationVeryRandom now, does it need orientation reward?"

n_envs: 10
num_evals_per_difficulty: 20
n_epochs: 5 # amount of training passes over the replay buffer per timestep
log_interval: 5
batch_size: 64


n_steps: 64 # amount of steps to collect per collect_rollouts per environment
# Tensorboard shows the mean_episode_length is below 3
# we can reduce the n_steps to make the collection time shorter while still collecting a lot of games

# maybe the fps is too low, the agent does not have many steps to do and cannot react quick enough to learn a better policy than full on ahead

copy_model_from: False #models_and_dumps/best_model_episode_70
# copy_model_from can be False or a string (filename without .zip suffix)

env_kwargs:
  jetbot: DifferentialJetBot
  spawn_point: OrientationVeryRandom
  # spawn_point can be Fixed, OrientationRandom, OrientationVeryRandom and FullyRandom
  single_goal: False
  frame_stacking: 3
  image_preprocessing:
    grayscale: True
    equalize: True
    contrast_increase: "TODO"
    # vielleicht eine art Mode variable hier reinpacken contrast increase s equalize
    normalize_images: False
  coefficients:
    distanceCoefficient: 0.5
    orientationCoefficient: 0.001
    velocityCoefficient: 0.0
    eventCoefficient: 10.0
  trainingMapType: randomEval
  width: 500 #336
  height: 168

