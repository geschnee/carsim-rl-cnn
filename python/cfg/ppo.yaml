comment: "Trying with more n_steps, does the performance still drop off after a while? does it drop off later?"
# we should also try it with the fixed spawn to be closer to the evaluation scenario, maybe the differential can do it 

n_envs: 10
num_evals_per_difficulty: 20
n_epochs: 5 # amount of training passes over the replay buffer per timestep
log_interval: 5
batch_size: 64


n_steps: 256 # 64 # amount of steps to collect per collect_rollouts per environment
# Tensorboard shows the mean_episode_length is below 3
# we can reduce the n_steps to make the collection time shorter while still collecting a lot of games

# maybe the fps is too low, the agent does not have many steps to do and cannot react quick enough to learn a better policy than full on ahead

copy_model_from: False #models_and_dumps/best_model_episode_70
# copy_model_from can be False or a string (filename without .zip suffix)

total_timesteps: 2500000

env_kwargs:
  jetbot: DifferentialJetBot
  spawn_point: OrientationRandom
  # spawn_point can be Fixed, OrientationRandom, OrientationVeryRandom and FullyRandom
  frame_stacking: 10
  image_preprocessing:
    grayscale: True
    equalize: True
    contrast_increase: "TODO"
    # vielleicht eine art Mode variable hier reinpacken contrast increase s equalize
    normalize_images: False
  coefficients:
    distanceCoefficient: 0.5
    orientationCoefficient: 0.0
    velocityCoefficient: 0.0
    eventCoefficient: 1.0
  trainingMapType: randomEval
  fixedTimestepsLength: False
  # fixedTimestepsLength can either be False or a number (seconds that each timestep contains)
  width: 500 #336
  height: 168

