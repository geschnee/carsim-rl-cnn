comment: "rewardFunction capability check, this tests if the agent is able to learn the individual reward function (for easy tracks), velocityReward"


# reducing n_envs results in higher fps (per env)
n_envs: 10
copy_model_from: False
# copy_model_from can be False or a string (filename without .zip suffix)

total_timesteps: 1000000

seed: 2048

eval_settings:
  interval_during_learn: False
  n_eval_episodes: 100
  eval_light_settings: True
  eval_only: False
  number_eval_runs: 1

episode_record_replay_settings:
  n_episodes_per_setting: 2
  deterministic_sampling: True
  replay_folder: False
  # replay_folder can be False to replay the previously recorded episodes

algo_settings:
  n_epochs: 5 # amount of training passes over the replay buffer per timestep


  batch_size: 64

  n_steps: 512 # 64 
# amount of steps to collect per collect_rollouts per environment
# Tensorboard shows the mean_episode_length is below 3
# we can reduce the n_steps to make the collection time shorter while still collecting a lot of episodes
# n_steps * fixedTimestepsLength = the amount of seconds required for the collection

  policy: "CnnPolicy"
  use_bundled_calls: True
  use_fresh_obs: False

  print_network_and_loss_structure: False

  net_arch:
    pi: []
    vf: []


env_kwargs:
  jetBotName: DifferentialJetBot
  spawnOrientation: Random
  frame_stacking: 10
  image_preprocessing:
    downsampling_factor: 2
    grayscale: True
    equalize: True
    normalize_images: False
  coefficients:
    distanceCoefficient: 0
    orientationCoefficient: 0
    velocityCoefficient: 1
    eventCoefficient: 0
  collisionMode: oncePerTimestep
  trainingMapType: randomEvalEasy
  trainingLightSetting: standard
  fixedTimestepsLength: 0.3

  width: 500 #800 #500 #336
  height: 168

